{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FP4 Quantization\n",
        "\n",
        "High-Level Overview of FP4 Quantization for LLMs\n",
        "FP4 (4-bit floating-point) quantization is a technique used to significantly reduce the memory footprint and computational cost of Large Language Models (LLMs). The core idea is to represent the high-precision floating-point numbers (like FP16 or FP32) that make up a model's weights and activations with a much smaller 4-bit floating-point format. This allows you to store a massive model in a fraction of the memory and perform computations faster.\n",
        "\n",
        "Here's a high-level breakdown of how it works:\n",
        "\n",
        "- The Challenge: LLMs are huge. A model like LLaMA-7B has 7 billion parameters, each typically stored as a 16-bit floating-point number. This requires roughly 14 GB of VRAM. This is a lot, and it limits who can run these models. The goal of quantization is to reduce this number.\n",
        "\n",
        "- The Idea: Instead of using 16 bits to represent each number, we'll use only 4 bits. A 4-bit floating-point number has a much smaller range of values it can represent. This is a trade-off: we save memory and compute, but we lose some precision. The challenge is to do this in a way that the model's performance doesn't degrade too much.\n",
        "\n",
        "The Core Process: Quantization and De-Quantization:\n",
        "\n",
        "- Quantization: When a model is loaded, its high-precision weights are converted to the 4-bit format. This involves a scaling factor and a data type conversion. The key is to find the right scaling factor that minimizes the loss of information.\n",
        "\n",
        "- De-Quantization (on-the-fly): During a forward pass (inference), the 4-bit weights are loaded from memory. However, to perform the actual matrix multiplication (the core operation in a Transformer's linear layer), the GPU's hardware often requires higher precision (e.g., FP16). So, the 4-bit weights are de-quantized back to a higher precision on the fly. The matrix multiplication is then performed in this higher precision, and the result is stored.\n",
        "\n",
        "- Handling Outliers: A major issue with quantizing LLMs is the presence of \"outliers.\" These are a few values in the weight or activation tensors that are much larger than the rest. A naive quantization scheme would be dominated by these outliers, making the rest of the values lose all their precision. Solutions like bitsandbytes' FP4 and NF4 handle this by using a small, high-precision representation for these outliers while quantizing the majority of the values to 4-bit. This is a \"mixed-precision\" approach within the 4-bit quantization."
      ],
      "metadata": {
        "id": "eJpvWp9f5Lhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key components of a bitsandbytes-like implementation are:\n",
        "\n",
        "## 4-bit Floating-Point Data Type\n",
        "bnb defined the NF4 - NormalFloat4. For our purpose let's use the standard FP4.\n",
        "In the FP4 format the data are represented as:\n",
        "\n",
        "|sign|exp|exp|mantissa| (E2M1)\n",
        "\n",
        "(-1)^s *(1+m/2)^(2-1) * 2^(exp-bias)\n",
        "\n",
        "sign: +1, -1\n",
        "mantissa: 0, 1\n",
        "exponent: 00, 01, 10, 11 (with bias=1)\n",
        "\n",
        "bias is pretty important because allow us to have negative exponents and managing subnormal numbers, that in deep learning are pretty important.\n",
        "\n",
        "So the total representable range is:\n",
        "\n",
        "[-1x(1+0.5)x(2^2)... +1x(1+0.5)x(2^2)]= [-6 ... 6]\n",
        "\n"
      ],
      "metadata": {
        "id": "_emA_FIB7jW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_E2M1:\n",
        "  '''\n",
        "  class that represent the E2M1 format\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.values = []\n",
        "    for sign in [0,1]:\n",
        "      for exp in range(2**2):\n",
        "        for mantissa in range(2):\n",
        "          if exp==0 and mantissa == 0:\n",
        "            value = 0\n",
        "          else:\n",
        "            exp_val = exp-1\n",
        "            mantissa_val = 1+mantissa*0.5\n",
        "            value = (1 if sign==0 else -1) * mantissa_val * (2**(exp_val))\n",
        "\n",
        "          if value not in self.values:\n",
        "            self.values.append(value)\n",
        "    self.values = sorted(self.values)"
      ],
      "metadata": {
        "id": "-dVzeWS-9T-B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case of E2M1\n",
        "fp4_range = FP4_E2M1()\n",
        "fp4_range.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVmibP1rE43c",
        "outputId": "76df3f69-8010-4147-faa9-3fccbda6c475"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-6.0,\n",
              " -4.0,\n",
              " -3.0,\n",
              " -2.0,\n",
              " -1.5,\n",
              " -1.0,\n",
              " -0.75,\n",
              " 0,\n",
              " 0.75,\n",
              " 1.0,\n",
              " 1.5,\n",
              " 2.0,\n",
              " 3.0,\n",
              " 4.0,\n",
              " 6.0]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "AjB56cCo6Hun"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_range = torch.tensor(FP4_E2M1().values)\n",
        "fp4_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIfrIkUZgYSd",
        "outputId": "746bb8ec-588d-4176-dcb9-5a0568d9c476"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.0000, -4.0000, -3.0000, -2.0000, -1.5000, -1.0000, -0.7500,  0.0000,\n",
              "         0.7500,  1.0000,  1.5000,  2.0000,  3.0000,  4.0000,  6.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Educational FP4 Quantization\n",
        "\n",
        "Very first version.\n",
        "This implementation quickly and simply describes the algorithm from an educational point of view.\n",
        "\n",
        "- The input tensor is taken and flattened to one dimension (flatten operation)\n",
        "\n",
        "- The scale value is computed as absmax()\n",
        "\n",
        "- The entire tensor is scaled\n",
        "\n",
        "- For each value in the tensor (using broadcasting), the closest value is calculated within the bucket of the allowed 4-bit value range\n",
        "\n",
        "- Finally, the quantized data is returned"
      ],
      "metadata": {
        "id": "JPGFcf_NKe1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_Quantizer():\n",
        "  def __init__(self):\n",
        "    self.fp4_values = torch.tensor(FP4_E2M1().values)\n",
        "  def quantize(self, input_tensor):\n",
        "    block = input_tensor.view(-1) # Flatten\n",
        "    scale = block.abs().max() # Get the max value of the block for the scale\n",
        "    if scale == 0:\n",
        "      return torch.zeros_like(block), scale\n",
        "    scaled_block =block/scale # Scale the tensor\n",
        "\n",
        "\n",
        "    indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.fp4_values),dim=1) # Find the nearest value from the range\n",
        "    quantized_data = self.fp4_values[indices]\n",
        "    return quantized_data, scale\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scale,original_shape):\n",
        "    t = quantized_tensor*scale\n",
        "    t = t.reshape(original_shape)\n",
        "    return t"
      ],
      "metadata": {
        "id": "_2BevkLr7RtZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer()"
      ],
      "metadata": {
        "id": "Dz4expWi7kcQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.randn((1,512))\n",
        "input_tensor.shape, input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5JKTapg9tZY",
        "outputId": "5cf912c1-b296-46b6-bf4f-366e491c9669"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 512]),\n",
              " tensor([[ 1.1179, -1.1281,  0.8373, -1.1588,  0.8053, -0.6481, -1.6126, -0.0629,\n",
              "          -0.2260, -1.3027,  0.4230,  0.1722, -0.7198, -1.8198, -0.5111, -0.5895,\n",
              "           0.4420, -0.3890,  1.2333, -0.1605,  1.6057, -0.1552,  1.6179, -0.3557,\n",
              "           0.0984,  0.5940, -0.8442, -0.6008,  1.6831, -1.2821,  0.1665,  0.4709,\n",
              "          -1.3341,  0.4199,  0.0672,  0.4475, -1.5926,  0.4505,  0.9490,  0.8768,\n",
              "           0.4062,  1.8352,  1.0064,  1.4580, -1.5108, -0.2318, -2.1338,  1.5809,\n",
              "           1.2413, -0.9667, -1.6481, -1.2794, -0.5184,  1.8801,  0.3109, -0.2874,\n",
              "          -0.1665, -1.0976, -0.6915,  0.6209, -0.4276, -0.5437, -0.0457, -1.4656,\n",
              "          -1.6254,  0.3077,  2.5372, -0.8167,  0.6427, -1.0736, -0.5634,  0.6819,\n",
              "           0.6419, -0.5678,  0.9447,  0.6653,  0.4357, -0.9539,  1.2157, -0.4664,\n",
              "          -0.0565,  0.4519, -0.2334, -0.4839, -0.4435, -0.4420,  0.6545,  0.8647,\n",
              "          -1.8317,  0.0663,  0.4506,  2.0528, -1.1950, -0.2536, -0.0069,  1.0724,\n",
              "          -1.5234,  1.2744, -0.1204,  0.6651, -0.1639, -0.2623,  0.2483,  0.0132,\n",
              "          -0.0608,  0.3404,  0.1190, -1.4559, -0.1616, -0.3602,  0.0802, -1.1793,\n",
              "          -0.7745,  0.5002, -1.4692, -0.5859,  1.0225,  0.6838, -0.2309, -0.6819,\n",
              "           0.1747,  0.1151, -0.3962,  0.4698,  1.3391, -1.6751,  0.0168,  1.7029,\n",
              "          -0.5731,  0.5485,  0.0200, -1.0273,  2.5496, -0.0307, -0.8080, -2.2132,\n",
              "          -1.1896, -1.0532,  0.6701,  0.3393, -1.3583, -1.0332, -0.7297,  0.2111,\n",
              "          -0.7094,  0.2091,  0.1100, -1.7715, -1.3297,  0.8887,  0.1688,  0.1020,\n",
              "           1.5422,  0.9256,  1.0833,  1.2042, -0.5817,  0.4068, -0.5880,  1.0555,\n",
              "          -0.8006,  1.5129,  0.2972,  1.1125,  0.0327, -1.0377, -0.8801, -0.5362,\n",
              "          -0.7245, -0.3471, -0.9596, -0.9690, -0.8548, -0.4522,  2.0865, -1.0303,\n",
              "          -0.5320,  0.2309, -0.9552, -0.4748,  0.3196, -0.0686,  0.3606, -0.2186,\n",
              "          -0.1406, -1.3726, -1.1229,  1.7616, -1.2065, -0.3603, -0.9685, -0.9515,\n",
              "          -0.3887,  0.2491,  0.1986,  1.5156, -1.1086, -1.0372,  0.5402,  0.4444,\n",
              "          -1.0467, -1.5507,  1.4957,  1.6430, -0.6982,  0.7674, -0.8960,  0.4647,\n",
              "           0.5110,  0.3054,  0.0185, -1.7534,  0.1526,  0.5852,  0.0909, -0.1060,\n",
              "           1.4844,  0.2735,  0.1271,  1.1333, -1.0898,  1.1358, -0.6397, -1.8841,\n",
              "           0.7375,  0.0318, -0.1204, -1.8289, -0.8079,  0.1441, -0.6122, -1.0654,\n",
              "           0.0905,  0.8340, -2.1245, -0.4407, -1.0630, -2.2980, -0.0092,  0.2040,\n",
              "           0.2043,  0.6013,  0.9098,  0.1671, -1.7672,  0.2505,  0.3007,  0.1023,\n",
              "          -0.4781,  0.5408,  1.2937, -0.1251,  0.2576,  0.3680, -1.3053, -1.0042,\n",
              "           0.9169, -0.0286, -1.2093,  2.1163,  0.1088, -1.4275, -0.4184,  0.4149,\n",
              "          -0.4407, -0.0107,  1.8097, -0.2596, -1.2980, -2.7847,  0.9725, -0.0737,\n",
              "           0.6364,  0.3745,  0.7831,  3.0774,  0.0118,  0.6414, -1.6777,  0.7730,\n",
              "          -0.2159,  2.9105, -2.6143, -1.1202,  0.5508,  0.0909, -0.5567,  1.1245,\n",
              "           1.7120, -0.4032,  0.7284,  0.4690,  0.1333,  1.5168,  2.0667, -0.9313,\n",
              "          -3.1223, -0.8985, -0.2682, -0.5495, -0.4520, -0.4633, -0.6426,  0.8094,\n",
              "          -0.3406, -0.6572,  1.3838,  2.0163,  1.5372,  1.4770, -0.0171,  1.2504,\n",
              "          -0.1667, -1.4892,  2.6061,  0.7174, -0.0114,  0.7289, -0.3105, -1.3705,\n",
              "          -0.9024, -2.1265,  0.3627,  0.6245, -0.3083,  0.4329, -0.1413,  1.4040,\n",
              "           0.4802, -1.0954,  1.0316,  1.3311,  0.7758,  1.4646,  1.6676,  0.6295,\n",
              "           0.3225, -0.4851, -0.1350, -0.2790,  0.5040,  0.2215, -0.0636, -1.1324,\n",
              "           0.6779,  0.7781,  3.6363, -0.5656, -0.3622,  0.6099,  0.7077,  0.3139,\n",
              "          -0.8165, -0.0840, -0.2438, -0.2831, -1.1296, -1.3284,  1.1874, -1.0391,\n",
              "          -0.6051, -0.8182, -0.7366,  0.4223,  0.5443, -0.0426, -0.3250,  0.2154,\n",
              "           1.0285, -0.7115, -0.1240,  0.6557, -0.5204,  1.7288,  0.5764, -1.0478,\n",
              "          -1.2081,  0.0294,  0.3171, -1.0245,  0.8363, -0.1018, -0.5237, -2.8592,\n",
              "           1.1979,  0.4163,  0.5570,  0.9018,  1.1038, -0.3217, -0.2427, -0.3736,\n",
              "           1.8874,  0.2209,  1.1986,  1.5533, -0.1363,  0.0320,  1.3746,  0.8513,\n",
              "           0.4330,  0.0667, -0.2236, -0.1005,  0.1884,  0.4804, -1.9656, -0.7951,\n",
              "           1.2442,  0.0206, -1.4069, -0.2165, -0.8645,  2.7126, -0.7577,  0.0245,\n",
              "          -0.1487,  0.6891, -0.1670,  0.9918,  0.9131, -0.2940, -1.7603,  0.7191,\n",
              "           0.1203,  2.0546, -0.4967,  0.6759,  1.2993, -0.6899, -0.5545, -1.0038,\n",
              "           0.3535,  1.7081, -0.2689,  1.9160, -0.1051, -0.4776,  0.2786,  0.2099,\n",
              "          -0.7604, -1.4800,  1.6188,  0.6741,  0.7091, -0.2664, -1.0930,  0.2855,\n",
              "          -1.1830, -0.4631,  0.6919, -1.0166, -0.7343, -0.0891,  0.5737, -0.7927,\n",
              "          -1.0789, -0.0854,  0.9693,  1.4475,  0.4844,  0.0806,  0.6547, -0.3351,\n",
              "          -1.4510,  0.9841, -1.6446, -0.1711,  0.3342, -0.3625, -2.0134,  0.6919,\n",
              "           1.5664, -1.5916, -0.0627, -0.3512, -1.0658, -0.7406,  0.4559, -0.1522,\n",
              "           0.5752,  0.2764, -1.5393,  1.1236, -0.0703, -0.4753, -0.0631, -0.0740,\n",
              "           0.1922, -0.7929,  1.3003,  1.4356, -0.8063, -0.0222,  1.0637,  0.7412,\n",
              "          -0.8186,  1.9631, -1.1206,  0.0272,  1.0365,  1.3622,  0.3955, -0.2150,\n",
              "          -1.9835, -0.9657, -1.0830, -0.1879,  2.1755,  1.3078, -0.2852, -0.6788]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_tensor, scale = quantizer.quantize(input_tensor=input_tensor)\n",
        "quantized_tensor, scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gORigRhX7n3p",
        "outputId": "876656f6-ee6a-4a97-c18e-0c5baeede3ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.7500, -0.7500,  0.0000, -0.7500,  0.7500,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "         -0.7500,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -0.7500,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.7500,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.7500,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.7500, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.7500,  0.0000,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.7500,  0.7500,  0.7500,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -0.7500,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.7500, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000]),\n",
              " tensor(3.6363))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor = quantizer.dequantize(quantized_tensor,scale, input_tensor.shape)"
      ],
      "metadata": {
        "id": "tHqaUL-h75XT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor-input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWxl3zUu_wCh",
        "outputId": "92154287-e4e3-4048-c310-4eb68b489f72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1179,  1.1281, -0.8373,  1.1588, -0.8053,  0.6481, -1.1146,  0.0629,\n",
              "          0.2260,  1.3027, -0.4230, -0.1722,  0.7198, -0.9075,  0.5111,  0.5895,\n",
              "         -0.4420,  0.3890, -1.2333,  0.1605,  1.1215,  0.1552,  1.1093,  0.3557,\n",
              "         -0.0984, -0.5940,  0.8442,  0.6008,  1.0441,  1.2821, -0.1665, -0.4709,\n",
              "          1.3341, -0.4199, -0.0672, -0.4475, -1.1347, -0.4505, -0.9490, -0.8768,\n",
              "         -0.4062,  0.8920, -1.0064,  1.2693, -1.2165,  0.2318, -0.5935,  1.1463,\n",
              "         -1.2413,  0.9667, -1.0792,  1.2794,  0.5184,  0.8471, -0.3109,  0.2874,\n",
              "          0.1665,  1.0976,  0.6915, -0.6209,  0.4276,  0.5437,  0.0457, -1.2617,\n",
              "         -1.1019, -0.3077,  0.1900,  0.8167, -0.6427,  1.0736,  0.5634, -0.6819,\n",
              "         -0.6419,  0.5678, -0.9447, -0.6653, -0.4357,  0.9539, -1.2157,  0.4664,\n",
              "          0.0565, -0.4519,  0.2334,  0.4839,  0.4435,  0.4420, -0.6545, -0.8647,\n",
              "         -0.8956, -0.0663, -0.4506,  0.6745,  1.1950,  0.2536,  0.0069, -1.0724,\n",
              "         -1.2039, -1.2744,  0.1204, -0.6651,  0.1639,  0.2623, -0.2483, -0.0132,\n",
              "          0.0608, -0.3404, -0.1190, -1.2714,  0.1616,  0.3602, -0.0802,  1.1793,\n",
              "          0.7745, -0.5002, -1.2581,  0.5859, -1.0225, -0.6838,  0.2309,  0.6819,\n",
              "         -0.1747, -0.1151,  0.3962, -0.4698, -1.3391, -1.0521, -0.0168,  1.0243,\n",
              "          0.5731, -0.5485, -0.0200,  1.0273,  0.1777,  0.0307,  0.8080, -0.5141,\n",
              "          1.1896,  1.0532, -0.6701, -0.3393,  1.3583,  1.0332,  0.7297, -0.2111,\n",
              "          0.7094, -0.2091, -0.1100, -0.9557,  1.3297, -0.8887, -0.1688, -0.1020,\n",
              "          1.1851, -0.9256, -1.0833, -1.2042,  0.5817, -0.4068,  0.5880, -1.0555,\n",
              "          0.8006,  1.2144, -0.2972, -1.1125, -0.0327,  1.0377,  0.8801,  0.5362,\n",
              "          0.7245,  0.3471,  0.9596,  0.9690,  0.8548,  0.4522,  0.6407,  1.0303,\n",
              "          0.5320, -0.2309,  0.9552,  0.4748, -0.3196,  0.0686, -0.3606,  0.2186,\n",
              "          0.1406, -1.3547,  1.1229,  0.9657,  1.2065,  0.3603,  0.9685,  0.9515,\n",
              "          0.3887, -0.2491, -0.1986,  1.2117,  1.1086,  1.0372, -0.5402, -0.4444,\n",
              "          1.0467, -1.1766,  1.2316,  1.0843,  0.6982, -0.7674,  0.8960, -0.4647,\n",
              "         -0.5110, -0.3054, -0.0185, -0.9739, -0.1526, -0.5852, -0.0909,  0.1060,\n",
              "          1.2428, -0.2735, -0.1271, -1.1333,  1.0898, -1.1358,  0.6397, -0.8431,\n",
              "         -0.7375, -0.0318,  0.1204, -0.8984,  0.8079, -0.1441,  0.6122,  1.0654,\n",
              "         -0.0905, -0.8340, -0.6028,  0.4407,  1.0630, -0.4292,  0.0092, -0.2040,\n",
              "         -0.2043, -0.6013, -0.9098, -0.1671, -0.9601, -0.2505, -0.3007, -0.1023,\n",
              "          0.4781, -0.5408, -1.2937,  0.1251, -0.2576, -0.3680,  1.3053,  1.0042,\n",
              "         -0.9169,  0.0286,  1.2093,  0.6109, -0.1088, -1.2998,  0.4184, -0.4149,\n",
              "          0.4407,  0.0107,  0.9175,  0.2596,  1.2980,  0.0574, -0.9725,  0.0737,\n",
              "         -0.6364, -0.3745, -0.7831, -0.3502, -0.0118, -0.6414, -1.0495, -0.7730,\n",
              "          0.2159, -0.1833, -0.1130,  1.1202, -0.5508, -0.0909,  0.5567, -1.1245,\n",
              "          1.0153,  0.4032, -0.7284, -0.4690, -0.1333,  1.2105,  0.6606,  0.9313,\n",
              "          0.3951,  0.8985,  0.2682,  0.5495,  0.4520,  0.4633,  0.6426, -0.8094,\n",
              "          0.3406,  0.6572,  1.3434,  0.7109,  1.1901,  1.2503,  0.0171, -1.2504,\n",
              "          0.1667, -1.2380,  0.1211, -0.7174,  0.0114, -0.7289,  0.3105, -1.3568,\n",
              "          0.9024, -0.6008, -0.3627, -0.6245,  0.3083, -0.4329,  0.1413,  1.3233,\n",
              "         -0.4802,  1.0954, -1.0316, -1.3311, -0.7758,  1.2627,  1.0597, -0.6295,\n",
              "         -0.3225,  0.4851,  0.1350,  0.2790, -0.5040, -0.2215,  0.0636,  1.1324,\n",
              "         -0.6779, -0.7781,  0.0000,  0.5656,  0.3622, -0.6099, -0.7077, -0.3139,\n",
              "          0.8165,  0.0840,  0.2438,  0.2831,  1.1296,  1.3284, -1.1874,  1.0391,\n",
              "          0.6051,  0.8182,  0.7366, -0.4223, -0.5443,  0.0426,  0.3250, -0.2154,\n",
              "         -1.0285,  0.7115,  0.1240, -0.6557,  0.5204,  0.9985, -0.5764,  1.0478,\n",
              "          1.2081, -0.0294, -0.3171,  1.0245, -0.8363,  0.1018,  0.5237,  0.1320,\n",
              "         -1.1979, -0.4163, -0.5570, -0.9018, -1.1038,  0.3217,  0.2427,  0.3736,\n",
              "          0.8399, -0.2209, -1.1986,  1.1739,  0.1363, -0.0320,  1.3527, -0.8513,\n",
              "         -0.4330, -0.0667,  0.2236,  0.1005, -0.1884, -0.4804, -0.7616,  0.7951,\n",
              "         -1.2442, -0.0206, -1.3204,  0.2165,  0.8645,  0.0147,  0.7577, -0.0245,\n",
              "          0.1487, -0.6891,  0.1670, -0.9918, -0.9131,  0.2940, -0.9670, -0.7191,\n",
              "         -0.1203,  0.6726,  0.4967, -0.6759, -1.2993,  0.6899,  0.5545,  1.0038,\n",
              "         -0.3535,  1.0191,  0.2689,  0.8112,  0.1051,  0.4776, -0.2786, -0.2099,\n",
              "          0.7604, -1.2473,  1.1085, -0.6741, -0.7091,  0.2664,  1.0930, -0.2855,\n",
              "          1.1830,  0.4631, -0.6919,  1.0166,  0.7343,  0.0891, -0.5737,  0.7927,\n",
              "          1.0789,  0.0854, -0.9693,  1.2798, -0.4844, -0.0806, -0.6547,  0.3351,\n",
              "         -1.2762, -0.9841, -1.0827,  0.1711, -0.3342,  0.3625, -0.7139, -0.6919,\n",
              "          1.1608, -1.1357,  0.0627,  0.3512,  1.0658,  0.7406, -0.4559,  0.1522,\n",
              "         -0.5752, -0.2764, -1.1880, -1.1236,  0.0703,  0.4753,  0.0631,  0.0740,\n",
              "         -0.1922,  0.7929, -1.3003,  1.2917,  0.8063,  0.0222, -1.0637, -0.7412,\n",
              "          0.8186,  0.7641,  1.1206, -0.0272, -1.0365, -1.3622, -0.3955,  0.2150,\n",
              "         -0.7437,  0.9657,  1.0830,  0.1879,  0.5517, -1.3078,  0.2852,  0.6788]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(dequantized_tensor-input_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTwe5ZZAolB",
        "outputId": "f7117ec7-2ee0-4211-88a5-cd703638f1a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0371)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see we have quantized and dequantize the original tensor, and of course we have a loss on the convertion. The average error is not that big in this example\n",
        "\n",
        "In some cases the differences are pretty huge, and this would be worst in case of bigger outliers"
      ],
      "metadata": {
        "id": "z4b2qagSbGKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NB This is really a basic educational implementation that doesn't really optimize the space for the quantization. It just to show the algorithm"
      ],
      "metadata": {
        "id": "D6WJGsisCObY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Blockwise Quantization\n",
        "\n",
        "This approach is simple and it's great, but real library use a more fine grain approach, calculating multiple scale factor base on blocks.\n",
        "\n",
        "Let's change our code to do that"
      ],
      "metadata": {
        "id": "dZqi5yB6dicT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.randn((1,512))\n",
        "input_tensor.shape, input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM_x8tCSiFVE",
        "outputId": "4be740e0-9e4f-433e-960d-e6da2c362c4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 512]),\n",
              " tensor([[-9.5547e-02, -1.3130e+00, -1.7122e-01, -3.0108e-02, -1.1359e-01,\n",
              "           2.7123e-01,  7.6356e-01, -3.8492e-01,  5.7544e-01, -1.2327e+00,\n",
              "          -4.5623e-01,  7.2639e-01,  6.8328e-01, -6.8503e-01, -1.4650e+00,\n",
              "           2.0754e-01,  6.3340e-01,  1.1460e+00, -2.4873e-01,  2.1266e+00,\n",
              "          -8.3623e-01, -4.1115e-01, -9.1674e-02,  1.3308e+00, -1.1053e-01,\n",
              "           2.7257e-01, -9.7877e-01,  8.8152e-01, -9.5816e-01, -3.7615e-01,\n",
              "          -6.4455e-01, -1.1638e+00, -3.0966e-01, -4.9553e-01, -1.2040e+00,\n",
              "           8.1451e-01,  2.3513e-01, -5.2868e-01, -1.1573e+00,  5.8821e-01,\n",
              "          -1.4447e+00, -2.7392e-02,  1.0972e+00, -1.4447e+00, -3.3108e-01,\n",
              "          -7.6031e-01, -1.1028e+00, -5.5641e-01,  1.2619e+00,  3.5575e-01,\n",
              "          -9.6245e-03, -3.6474e-01,  2.9966e-01,  9.1289e-02,  2.3315e-01,\n",
              "          -1.4713e+00,  1.3958e+00,  6.4976e-01, -9.4165e-01, -2.5871e-01,\n",
              "          -3.4844e-01,  1.6903e+00,  7.8741e-01, -2.3817e+00, -4.4762e-01,\n",
              "           1.1931e+00, -6.8003e-02,  4.5626e-01,  1.4483e+00,  3.2315e-01,\n",
              "           7.8105e-01, -1.1873e+00, -2.0320e-01, -4.4784e-01,  1.5544e+00,\n",
              "          -1.5406e+00,  1.4298e+00,  2.0443e-01,  1.3870e+00, -1.8071e+00,\n",
              "          -1.6622e+00,  4.2402e-01,  1.3055e+00, -6.1794e-02,  8.6409e-01,\n",
              "          -1.2423e+00, -1.1558e+00,  5.3399e-01,  3.8144e-01, -9.7494e-01,\n",
              "           2.2868e-02, -1.0480e+00, -2.2232e+00,  7.2926e-01, -3.5445e-01,\n",
              "           8.9904e-01, -1.5069e-01,  9.4752e-01,  1.0776e+00,  5.4520e-01,\n",
              "           2.2089e-02, -1.9614e+00, -6.6888e-01,  9.5254e-01, -1.1137e+00,\n",
              "          -9.1454e-01, -1.0804e+00, -9.3907e-01, -5.2322e-01, -4.7187e-01,\n",
              "          -1.1717e+00,  8.7971e-01,  1.0630e+00, -1.4525e+00,  7.3615e-01,\n",
              "          -5.6343e-01, -4.1171e-01, -1.2739e+00, -8.6370e-01, -3.7489e-01,\n",
              "           1.0726e-01, -1.7126e+00,  1.5054e+00, -2.1353e+00, -8.5833e-01,\n",
              "           4.6225e-02, -1.1826e-01,  1.2423e+00, -1.7132e-01,  3.5920e-01,\n",
              "          -5.3817e-01,  1.8208e-01, -1.3938e+00, -4.3860e-01, -2.4268e-01,\n",
              "           1.0227e+00, -4.9488e-03, -3.3218e-01,  5.2227e-02, -1.1279e+00,\n",
              "           7.8885e-01, -2.3710e-04, -5.0205e-01,  9.6512e-01,  1.9299e+00,\n",
              "          -9.0330e-01,  8.7478e-01, -7.7416e-01,  7.8150e-01,  6.7740e-01,\n",
              "           4.9673e-01, -2.7474e-01,  1.0457e+00,  2.4607e-01, -1.6406e+00,\n",
              "          -1.5851e+00, -8.2803e-01,  8.4588e-01, -2.4571e-01, -2.6731e-01,\n",
              "           3.5036e-01, -1.0774e-01, -1.4288e+00,  2.2140e+00,  4.7395e-01,\n",
              "           8.3978e-01, -6.0464e-01, -5.6636e-01, -7.0441e-03, -1.9883e-01,\n",
              "          -2.2854e-01,  1.1689e+00,  7.0014e-01,  1.0129e+00,  2.9210e-01,\n",
              "          -8.4861e-01, -1.6596e+00, -1.4556e-01, -1.4807e-01, -7.4677e-02,\n",
              "          -9.3802e-01,  6.5454e-01, -1.4353e+00, -2.0087e+00, -3.6064e-02,\n",
              "           6.4579e-01, -2.4770e-01, -7.6374e-01,  8.2822e-02,  8.2842e-01,\n",
              "          -2.0484e+00,  8.0955e-01,  1.2597e+00,  8.8586e-02,  6.3413e-01,\n",
              "           4.6837e-01,  4.5663e-01, -1.1131e+00, -4.0334e-01,  3.8153e-01,\n",
              "          -5.5399e-01,  8.2126e-01, -2.4393e-01,  1.3438e+00,  9.7578e-01,\n",
              "           2.6360e-01, -1.9035e+00,  1.8076e+00,  1.2347e+00,  1.9033e+00,\n",
              "           1.7834e-01, -1.5536e+00,  4.1302e-01,  5.2802e-03, -2.5723e-01,\n",
              "           6.6127e-01, -1.2962e+00,  8.9538e-01, -1.6504e+00,  1.5115e+00,\n",
              "           6.0170e-01,  1.8125e+00, -1.2419e+00,  1.7635e-01,  2.1929e+00,\n",
              "          -3.7611e-01, -1.0385e+00, -1.1417e+00, -1.1404e+00,  4.8050e-01,\n",
              "          -1.7708e+00, -1.4305e+00, -8.3722e-01,  1.3104e+00, -4.0836e-03,\n",
              "           1.0777e+00,  7.3162e-01, -8.2845e-02,  1.8350e+00, -4.8457e-01,\n",
              "          -2.6282e+00, -1.3283e+00,  1.3725e-01, -1.2017e+00, -1.0333e+00,\n",
              "          -9.0841e-01, -5.1978e-01,  1.0708e+00,  2.2284e+00, -1.8774e-01,\n",
              "          -7.0145e-01, -1.5304e+00,  5.1456e-01, -3.8570e-01, -1.2232e+00,\n",
              "           1.8179e+00, -8.6803e-01, -1.2758e+00, -7.2597e-01, -1.5815e+00,\n",
              "           5.7811e-02, -4.9336e-01, -3.8464e-01, -1.8888e-01, -1.3240e+00,\n",
              "          -6.8002e-01,  1.4035e+00,  4.2589e-01,  2.0317e+00,  2.9362e-02,\n",
              "          -1.5075e+00,  1.0901e+00, -3.8449e-01, -8.0909e-02,  2.2066e-01,\n",
              "          -9.2501e-01,  3.6292e-01, -1.2950e+00, -1.0625e+00,  8.8931e-01,\n",
              "           3.1864e-01, -3.1224e-01,  1.5128e+00,  8.2368e-01,  2.6795e-01,\n",
              "          -1.8681e+00, -9.7876e-01, -4.5438e-01, -9.4072e-01,  6.1663e-01,\n",
              "          -6.1684e-01,  3.3442e-03, -2.7259e-01, -2.0739e-01,  3.1240e-01,\n",
              "          -6.8488e-02, -1.1055e+00,  4.6147e-01,  5.7625e-01,  2.1053e-01,\n",
              "          -1.0224e+00, -2.9736e-02,  4.2204e-01, -2.8780e+00,  6.5998e-01,\n",
              "           1.3975e+00, -1.3794e+00,  2.7872e+00, -4.1589e-01, -6.2944e-02,\n",
              "          -1.8393e+00,  7.0664e-01,  1.1728e+00, -1.1227e+00,  3.1352e-02,\n",
              "          -4.7735e-01,  1.1951e+00, -8.8805e-01, -2.1227e+00, -6.7849e-01,\n",
              "           2.3351e-01,  1.4620e+00,  2.6927e-01,  2.1549e-01, -3.6077e-01,\n",
              "          -2.3661e-01,  2.0199e+00, -1.1653e+00, -2.3945e+00,  3.1868e-01,\n",
              "          -1.8045e-01, -1.9624e+00,  3.6425e-01, -4.5982e-01, -1.0859e+00,\n",
              "          -1.0164e+00,  9.9832e-01, -3.2248e-01, -8.6853e-01, -3.2573e-01,\n",
              "          -1.9810e-01,  8.8017e-01,  6.4320e-01,  7.9009e-02,  1.8248e+00,\n",
              "          -4.6742e-01,  5.5722e-01,  4.2113e-01,  5.6705e-02, -4.4212e-01,\n",
              "          -1.1573e+00,  8.2209e-01, -1.2514e+00, -2.0170e+00,  3.8661e-01,\n",
              "          -2.5932e+00, -7.2449e-01, -1.9205e+00,  4.3017e-02, -6.9650e-01,\n",
              "          -4.0204e-01, -1.3023e-01,  2.1958e+00, -2.3168e+00, -1.6896e-01,\n",
              "          -1.0318e+00,  1.7613e-01,  2.9527e-01,  1.8252e-01,  9.7986e-01,\n",
              "           1.0086e+00, -2.5419e-02,  3.2834e-02,  1.5954e-01,  4.4954e-01,\n",
              "           5.4740e-01,  1.4730e-01, -1.1091e+00,  3.5226e-01, -1.2033e+00,\n",
              "          -9.0088e-02, -5.3842e-01, -2.8363e+00,  1.4299e+00,  1.1965e-01,\n",
              "          -2.0192e-01,  1.1856e-01, -9.5622e-01, -1.4957e+00,  8.3058e-01,\n",
              "           9.8158e-01,  2.9027e-01, -4.9889e-01, -1.8632e-01, -1.6851e-01,\n",
              "          -1.7292e+00,  8.0234e-01, -1.2474e+00,  3.5241e-01, -1.4782e+00,\n",
              "          -1.0792e+00,  3.3573e-01, -2.5711e-01, -3.2900e-01, -4.0064e-01,\n",
              "           7.5706e-01,  8.8128e-01, -7.0544e-01, -6.5298e-02, -1.0173e+00,\n",
              "          -2.7176e-03, -1.4603e+00,  2.0531e+00,  5.7091e-01, -1.8861e-01,\n",
              "          -4.0624e-01,  1.1195e-02, -8.3454e-02,  6.4356e-01, -1.3694e-01,\n",
              "           7.2186e-01, -4.8421e-01, -5.1255e-01,  6.2604e-01,  6.1065e-01,\n",
              "           3.1744e-01,  1.8526e-01, -2.0309e+00, -1.1082e+00, -2.6999e-02,\n",
              "          -1.1782e-01, -5.3183e-01, -1.7625e-01, -1.5495e+00, -2.2440e-01,\n",
              "           1.1202e+00,  1.6765e+00,  2.0145e+00, -1.4765e+00,  6.6300e-01,\n",
              "           4.5260e-01, -1.4473e+00,  1.2744e+00,  2.0561e+00, -6.7138e-01,\n",
              "          -5.3957e-01,  6.6671e-01, -6.5346e-01, -4.0829e-02, -3.4647e-01,\n",
              "          -1.2741e+00, -2.1649e+00,  7.2919e-01,  1.4062e+00,  1.2356e-01,\n",
              "           4.8872e-01,  3.3006e-01,  1.0089e+00,  1.7741e+00, -9.4649e-01,\n",
              "          -1.1998e+00, -2.5271e-01,  9.2346e-02, -3.2450e-01,  4.0185e-01,\n",
              "           6.5361e-01,  1.2204e+00,  1.6477e+00,  3.5849e-02, -9.1723e-01,\n",
              "          -3.3345e-01,  5.2703e-01, -1.2684e+00, -8.9403e-01,  8.1066e-01,\n",
              "          -6.8162e-01, -6.7764e-01, -1.1258e+00,  3.2465e-01,  2.2761e-01,\n",
              "          -5.2300e-02, -2.7196e-01, -2.1177e+00, -1.2666e+00, -1.5482e-01,\n",
              "           5.0581e-01,  1.0578e+00, -1.6188e+00,  9.5761e-01, -2.3820e+00,\n",
              "           5.8672e-01, -1.0503e-02,  1.3106e+00, -9.1489e-02, -2.1474e-01,\n",
              "           8.1028e-01, -1.4707e+00,  1.2264e+00, -1.2744e+00,  9.2302e-01,\n",
              "          -2.0113e+00, -5.6140e-01, -3.2740e-01,  1.7223e+00, -1.0466e+00,\n",
              "           5.2835e-01,  1.4156e-01, -1.1921e+00,  1.5097e-01, -1.6903e+00,\n",
              "           1.3299e+00, -1.7835e-01]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTUcx4doDDl",
        "outputId": "e1038eef-4abb-4eb1-85c2-2e50707dbd96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.0000, -4.0000, -3.0000, -2.0000, -1.5000, -1.0000, -0.7500,  0.0000,\n",
              "         0.7500,  1.0000,  1.5000,  2.0000,  3.0000,  4.0000,  6.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_Quantizer_Blockwise():\n",
        "  def __init__(self,block_size=8):\n",
        "    self.fp4_values = torch.tensor(FP4_E2M1().values)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def quantize(self,input_tensor):\n",
        "    data_flat = input_tensor.view(-1) # Flatten\n",
        "    num_blocks = (data_flat.numel()+ self.block_size -1) // self.block_size\n",
        "    quantized_data = torch.zeros(num_blocks * (self.block_size//2), dtype=torch.uint8) # Every 8 bit we'll pack together 2 tensor of 4 bit\n",
        "    scales = torch.zeros(num_blocks)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      start = i*self.block_size\n",
        "      end = min((i+1)*self.block_size,data_flat.numel())\n",
        "      block = data_flat[start:end]\n",
        "      scale = block.abs().max() # Get the max value of the block for the scale\n",
        "      if scale == 0:\n",
        "        scale = 1.0\n",
        "      scales[i] = scale # Saving the scale factor for the block\n",
        "\n",
        "      scaled_block = block/scale # Scale the tensor\n",
        "      indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.fp4_values),dim=1) # Find the nearest value\n",
        "      # Combine two 4 bit indices in one uint8 value\n",
        "      # This operation refactor the indices organizing it in group of two [[1,2],[3,4]...]\n",
        "      # Then pack the values of the first column with the second column moving this one 4bit to the left (left bit shift operator)\n",
        "      # For example if the index is 5 (0101) shifting it left will result in 0101 0000 (80)\n",
        "      if indices.numel() % 2 != 0:\n",
        "        # Pad with a dummy value to make the number of elements even\n",
        "        indices = torch.cat((indices, torch.tensor([0], dtype=indices.dtype)))\n",
        "\n",
        "      packed_indices = indices.view(-1,2)\n",
        "      packed_values = packed_indices[:, 0] | (packed_indices[:, 1] << 4)\n",
        "      quantized_data[i * (self.block_size // 2) : i * (self.block_size // 2) + packed_values.numel()] = packed_values\n",
        "    return quantized_data, scales\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scales, original_shape):\n",
        "    num_elements = torch.prod(torch.tensor(original_shape))\n",
        "    dequantized_flat = torch.zeros(num_elements, dtype=torch.float32)\n",
        "\n",
        "    num_blocks = scales.numel()\n",
        "    current_index = 0\n",
        "    for i in range(num_blocks):\n",
        "      start = i * self.block_size\n",
        "      end = min((i+1)*self.block_size, num_elements)\n",
        "      current_block_size = end-start\n",
        "      # How many 8-bit values to unpack for the current block\n",
        "      packed_block_size = (current_block_size + 1) // 2\n",
        "      packed_values = quantized_tensor[current_index:current_index+packed_block_size]\n",
        "      # Unpack the values -> I need to do a bitwise operation the most signifanct bit will be the second index\n",
        "      # The least significant bits will be the first index\n",
        "\n",
        "      index_1 = packed_values & 0x0F\n",
        "      index_2 = (packed_values >> 4) & 0x0F\n",
        "      indices_unpacked =torch.stack([index_1,index_2], dim=1).view(-1)\n",
        "      indices_unpacked = indices_unpacked[:current_block_size]\n",
        "      fp4_block_value = self.fp4_values[indices_unpacked.long()]\n",
        "      dequantized_flat[start:end] = fp4_block_value * scales[i]\n",
        "      current_index += packed_block_size\n",
        "    return dequantized_flat.view(original_shape)"
      ],
      "metadata": {
        "id": "sPlRWjcaA61S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer_Blockwise(block_size=64)"
      ],
      "metadata": {
        "id": "VuUlTmxqBpXL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_data, scales = quantizer.quantize(input_tensor)"
      ],
      "metadata": {
        "id": "CrMsujC6Bz1Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_data, len(scales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKmouX5UCILH",
        "outputId": "03134cae-cc96-4dcc-cc6e-e5728db8c9b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([103, 119, 119, 119, 103, 119, 119, 118, 135, 151, 119, 135, 119, 118,\n",
              "         118, 103, 119, 118, 119, 118, 118, 104, 119, 118, 120, 119, 119, 103,\n",
              "         120, 118, 135,  87, 135, 119, 120, 103, 119, 104, 120, 104, 118, 120,\n",
              "         104, 118, 103, 103, 117, 135, 135, 120,  87, 135, 102, 102, 119, 134,\n",
              "         104, 119, 103, 118, 103,  88, 118, 135, 119, 119, 118, 135, 119, 103,\n",
              "         119, 135, 104, 120, 119, 119, 120, 102, 135, 119, 119, 150, 135, 119,\n",
              "         119, 135, 135, 103, 118, 119, 118,  86, 119, 119, 119, 117, 120, 119,\n",
              "         103, 119, 119, 135, 119, 134, 136, 103, 119, 119, 118, 134, 135, 118,\n",
              "         120, 102, 118, 102, 135, 135, 119, 120, 101, 103, 118, 135, 120, 103,\n",
              "         119, 134, 103, 103, 119, 119, 118, 120, 120, 134, 119, 119, 103, 119,\n",
              "         119, 120, 103, 119, 119, 119, 119, 119, 118, 119, 119,  87, 135, 150,\n",
              "         119, 118, 104, 119, 120, 118, 135, 119, 119, 104, 118, 103, 119, 118,\n",
              "         119, 119, 119, 119, 120, 119, 119, 118, 102,  87, 103, 119, 119, 104,\n",
              "         119, 119, 119, 119, 119, 119, 103, 103, 119, 133, 119, 103, 134, 120,\n",
              "         119, 103, 104, 103, 118, 119, 119, 120, 103, 103, 121, 119, 119, 119,\n",
              "         119, 119, 119,  87, 118, 119, 103, 135, 152, 118, 103, 152, 119, 119,\n",
              "         119,  86, 135, 119, 135, 104, 118, 119, 119, 136, 103, 119, 102, 119,\n",
              "         103, 119, 119, 101, 119, 104,  88, 119, 120, 119, 134, 134, 118, 135,\n",
              "         118, 103, 103, 120], dtype=torch.uint8),\n",
              " 8)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have N scale factor (with N =Tensor Input dimension / block_size)"
      ],
      "metadata": {
        "id": "SrnIyYAPCquy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor = quantizer.dequantize(quantized_data,scales, input_tensor.shape)"
      ],
      "metadata": {
        "id": "uespBbDNC0Ve"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_tensor- input_tensor).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jEoYH4DACq",
        "outputId": "120ee3e0-e608-4ea6-83a5-a89607bffb0b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4305)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matmul operations\n",
        "\n",
        "Let's see a problem using the FP4 data type"
      ],
      "metadata": {
        "id": "vUzVNOexDUO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 64"
      ],
      "metadata": {
        "id": "FscvpvuUOqGA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class matmul():\n",
        "  def __init__(self,quantizer):\n",
        "    self.quantizer = quantizer\n",
        "  def __call__(self, input_tensor, weights, scales=None, weights_quantized=False, shape=None):\n",
        "    if weights_quantized:\n",
        "      if shape is None or scales is None:\n",
        "        raise Exception(\"'shape' and 'scales' are required\")\n",
        "      weights = self.quantizer.dequantize(weights, scales, shape)\n",
        "      weights = weights.to(torch.bfloat16)\n",
        "    output = torch.matmul(input_tensor, weights.T)\n",
        "    return output"
      ],
      "metadata": {
        "id": "K90cZWe4OiMh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer_Blockwise(block_size=BLOCK_SIZE)"
      ],
      "metadata": {
        "id": "a3K1p7T5Dv_u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features, out_features = 1024, 512\n",
        "weights = torch.randn(out_features, in_features).to(torch.bfloat16)\n",
        "input_tensor = torch.randn(1, in_features).to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "alca_32YDTkR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matmul_operation = matmul(quantizer = quantizer)"
      ],
      "metadata": {
        "id": "I4u4twe9JwpB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_matmul_result = matmul_operation(input_tensor, weights, weights_quantized=False)"
      ],
      "metadata": {
        "id": "jERTvbLJKBMu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)\n",
        "dequantized_matmul_result = matmul_operation(input_tensor, quantized_weight, weights_quantized=True,scales=scales, shape=weights.shape)"
      ],
      "metadata": {
        "id": "jdBEw6sDDlkO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_matmul_result - base_matmul_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STPY27ZcESNK",
        "outputId": "e3e26d89-d88b-4fa1-c073-c642519ce7b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.4812e+01, -1.0625e+01, -2.1000e+01,  4.6000e+01,  9.2500e+00,\n",
              "          8.9375e+00,  1.6125e+01,  1.0500e+01,  2.6250e+00, -2.7188e+00,\n",
              "         -1.5625e+01,  2.3000e+01, -2.1719e+00, -8.7500e-01, -2.3000e+01,\n",
              "          0.0000e+00, -4.6875e+00, -4.0750e+01,  2.5000e+00, -2.4000e+01,\n",
              "          2.8750e+01, -2.1375e+01, -2.4000e+01,  5.1250e+01,  3.9500e+01,\n",
              "         -1.5500e+01, -7.3750e+00, -7.9688e+00, -5.8438e+00,  5.2500e+00,\n",
              "         -4.3750e-01, -2.7250e+01,  5.4375e+00, -4.9000e+01,  3.1250e+00,\n",
              "         -1.7250e+01,  6.5000e+00, -1.2500e+00,  1.2125e+01, -7.7500e+00,\n",
              "         -1.5875e+01, -9.5000e+00,  1.8000e+01,  8.7500e-01, -2.1750e+01,\n",
              "          2.0000e+00, -6.0000e+00, -2.3875e+01,  1.8375e+01,  2.6375e+01,\n",
              "          2.0250e+01,  4.5000e+00, -9.5000e+00, -8.9375e+00, -1.7625e+01,\n",
              "          4.0750e+01,  8.6250e+00,  1.7000e+01, -4.5000e+00, -1.0812e+01,\n",
              "         -2.8750e+00, -1.6250e+01,  5.5000e+00, -2.8875e+01,  1.2688e+01,\n",
              "         -7.6875e+00,  2.7500e+01,  6.0938e+00,  2.3875e+01, -5.2500e+00,\n",
              "         -3.0250e+01, -9.8125e+00, -2.9750e+01,  1.0000e+00, -2.7875e+01,\n",
              "         -2.3750e+00, -2.5250e+01,  2.3500e+01,  1.2500e+01,  1.1938e+01,\n",
              "         -1.4938e+01, -8.5000e+00, -2.6500e+01, -2.0125e+01, -6.2500e+00,\n",
              "          3.2250e+01,  2.3625e+01,  9.2500e+00,  3.3500e+01, -2.5000e+00,\n",
              "         -3.2000e+01,  3.6250e+00, -1.8750e+01, -2.0000e+00,  1.4750e+01,\n",
              "         -2.5375e+01, -1.2562e+01, -1.6875e+01,  3.3750e+00, -2.0750e+01,\n",
              "          8.9375e+00, -2.1875e+00,  7.5625e+00,  1.7250e+01, -1.7875e+01,\n",
              "          2.2500e+00,  1.0750e+01,  4.0000e+00,  2.0000e+01,  1.3250e+01,\n",
              "          1.9000e+01,  2.1250e+01, -1.7250e+01,  8.8750e+00, -1.8500e+01,\n",
              "         -4.7500e+00,  2.1250e+00,  3.1250e+01, -1.8375e+01,  1.0000e+01,\n",
              "         -3.7000e+01, -3.8500e+01,  2.8375e+01, -3.6250e+01,  2.5000e+00,\n",
              "         -2.3125e+01,  9.8750e+00,  1.0750e+01,  4.3000e+01,  2.6750e+01,\n",
              "         -1.3000e+01,  2.9500e+01,  5.3500e+01, -1.7375e+01, -6.2500e-02,\n",
              "          9.1875e+00,  8.2500e+00, -7.9375e+00,  8.0000e+00,  1.1250e+01,\n",
              "          5.7500e+00,  2.3000e+01, -5.2500e+00,  2.2500e+00,  7.2500e+00,\n",
              "          4.5000e+00,  1.7500e+00, -1.6000e+01,  1.2625e+01,  1.5375e+01,\n",
              "          2.6250e+01,  2.4875e+01,  2.6000e+01,  1.0000e+01, -3.7500e+00,\n",
              "          1.5938e+01, -5.8750e+00, -3.0000e+01,  2.5000e+01, -2.5500e+01,\n",
              "          1.1250e+01, -9.7500e+00,  4.1875e+00, -5.3750e+00, -1.9062e+00,\n",
              "         -2.6625e+01, -2.1250e+01, -3.1250e+00,  1.4500e+01,  1.7250e+01,\n",
              "          1.5625e+00,  2.4875e+01, -1.5688e+01,  1.4312e+01, -1.3312e+01,\n",
              "         -7.7500e+00, -3.2500e+01,  9.7500e+00, -8.0625e+00,  3.5000e+00,\n",
              "         -8.3125e+00, -1.3500e+01,  3.0125e+01,  1.3250e+01, -8.3750e+00,\n",
              "         -1.1250e+00,  7.0625e+00, -1.5000e+00, -3.3750e+01,  1.0250e+01,\n",
              "         -2.0500e+01, -1.8625e+01,  1.4000e+01, -1.4062e+00, -1.6500e+01,\n",
              "          6.1875e+00, -4.6250e+00,  0.0000e+00, -6.7500e+00,  7.6250e+00,\n",
              "         -2.7875e+01, -2.3750e+01, -1.1875e+01, -2.4375e+01, -8.5000e+00,\n",
              "         -5.1000e+01,  2.0500e+01,  8.5000e+00, -1.0000e+00,  1.2000e+01,\n",
              "          8.0625e+00, -2.6250e+01, -3.7500e+00, -3.6000e+01, -1.3625e+01,\n",
              "          4.1250e+00,  1.1250e+01, -2.2812e+00, -5.0000e+00, -2.4000e+01,\n",
              "         -7.1000e+01,  1.5000e+00, -1.3750e+01,  3.7250e+01, -3.4500e+01,\n",
              "          2.5000e-01,  9.7500e+00,  3.4000e+01, -6.2500e+00, -2.2250e+01,\n",
              "          1.1562e+00, -5.0000e+00, -6.3750e+00,  1.8750e+01, -9.6250e+00,\n",
              "          1.1125e+01, -1.6125e+01, -2.5750e+01,  5.9375e+00, -6.0000e+00,\n",
              "         -1.0000e+01,  9.0625e+00, -3.1500e+01, -3.7500e-01, -2.5000e-01,\n",
              "          9.7500e+00,  1.4250e+01,  3.1250e+00,  2.9875e+01,  3.8750e+00,\n",
              "         -8.5000e+00,  5.0000e+00, -1.7812e+00,  1.6625e+01,  1.7625e+01,\n",
              "         -2.8000e+01, -2.9500e+01,  4.7500e+00,  1.5250e+01, -2.4500e+01,\n",
              "          7.1250e+00,  2.4375e+01, -1.3250e+01,  2.6562e+00, -4.0000e+00,\n",
              "         -1.8000e+01,  1.9500e+01, -3.7250e+01, -2.6250e+00, -1.1062e+01,\n",
              "          2.2125e+01,  1.5000e+00,  3.9500e+01,  1.6250e+00, -3.5500e+01,\n",
              "         -2.6500e+01,  1.1875e+01, -1.5812e+01,  1.9625e+01,  1.4500e+01,\n",
              "          1.1250e+01, -8.3750e+00,  3.1250e+00,  1.5750e+01,  1.3000e+01,\n",
              "         -7.0000e+00,  7.4062e+00,  0.0000e+00, -1.0062e+01,  9.1250e+00,\n",
              "         -7.2500e+00, -9.3750e+00,  1.1500e+01, -4.8750e+00, -3.9250e+01,\n",
              "         -5.6000e+01, -1.8250e+01, -1.3312e+01,  2.2500e+00,  1.7125e+01,\n",
              "         -1.5250e+01, -6.0000e+00, -3.5000e+00, -3.4000e+01,  3.1250e+01,\n",
              "          1.6500e+01, -4.1500e+01, -3.8500e+01,  1.7625e+01,  2.3250e+01,\n",
              "         -4.5000e+00, -1.3188e+01, -1.0625e+01,  9.5000e+00, -3.6250e+00,\n",
              "         -1.4750e+01,  9.7500e+00, -2.3750e+00, -3.0000e+00, -2.2500e+00,\n",
              "         -1.8000e+01,  4.2500e+00, -4.7500e+00,  6.2500e-01, -1.4312e+01,\n",
              "         -2.4875e+01,  1.7125e+01, -1.3938e+01,  2.4375e+00,  2.0000e+00,\n",
              "         -2.0000e+00,  2.0500e+01,  7.3750e+00, -1.5250e+01, -2.5500e+01,\n",
              "         -1.5750e+01, -1.7750e+01,  2.2625e+01, -2.8750e+01, -1.5625e+01,\n",
              "         -3.8750e+00,  1.4750e+01, -1.9000e+01,  7.4375e+00,  1.9375e+01,\n",
              "         -3.7500e+00,  1.2500e+00, -1.9375e+01,  3.1250e+00, -1.9875e+01,\n",
              "         -2.5875e+01,  2.3250e+01,  1.7125e+01, -3.0625e+00, -1.5000e+01,\n",
              "          6.2500e-01,  7.1250e+00, -5.2500e+00,  3.3000e+01,  1.7625e+01,\n",
              "         -2.3000e+01,  1.5688e+01,  1.5688e+01,  1.3625e+01,  1.4625e+01,\n",
              "         -7.5000e+00,  5.3125e+00,  9.6250e+00, -1.9250e+01,  2.1500e+01,\n",
              "         -1.9500e+01, -6.5000e+00,  1.4625e+01,  2.1500e+01,  1.4688e+01,\n",
              "         -1.6625e+01,  3.7344e+00,  2.7750e+01,  7.5000e-01, -4.0000e+00,\n",
              "         -2.0250e+01,  2.0000e+01,  7.6250e+00, -1.0500e+01, -2.2500e+00,\n",
              "         -2.2250e+01,  4.3000e+01, -2.4875e+01, -1.3562e+01,  3.0000e+00,\n",
              "         -2.4750e+01,  2.1500e+01,  9.0000e+00, -1.8750e+00,  1.1500e+01,\n",
              "          1.1250e+01,  1.1250e+01,  1.8500e+01,  2.8125e+00,  0.0000e+00,\n",
              "         -1.4250e+01, -2.0375e+01,  3.2000e+01,  1.2000e+01,  1.2125e+01,\n",
              "          2.4750e+01, -1.2500e+00,  1.0500e+01, -9.2500e+00,  8.1250e+00,\n",
              "         -6.5000e+00,  5.0000e+00, -1.1125e+01, -3.1250e+01, -3.1125e+01,\n",
              "          9.3750e+00,  2.1000e+01,  2.0000e+00, -1.2500e-01,  1.1688e+01,\n",
              "         -2.6000e+01, -6.1875e+00,  2.6250e+00, -2.3125e+01,  4.3750e+00,\n",
              "          1.2500e-01,  4.5000e+00,  2.0500e+01,  1.4000e+01, -1.0000e+00,\n",
              "          5.3750e+00, -1.2875e+01,  2.2250e+01, -1.5000e+00, -2.9500e+01,\n",
              "          3.0125e+01, -2.0000e+00, -1.3500e+01,  1.8000e+01,  1.2500e+00,\n",
              "         -1.3000e+01, -3.2000e+01, -6.6250e+00,  1.0500e+01, -1.5500e+01,\n",
              "         -5.8750e+00,  1.2000e+01, -3.0750e+01, -9.4375e+00, -6.5625e+00,\n",
              "          3.5000e+00,  1.0375e+01, -9.6250e+00,  1.7750e+01,  2.7500e+00,\n",
              "          1.5875e+01,  8.1250e+00,  1.6250e+00,  2.0000e+01,  3.2500e+00,\n",
              "          2.3750e+00, -2.5250e+01, -4.8750e+00, -2.2500e+00,  5.9688e+00,\n",
              "          1.3188e+01,  2.2750e+01, -1.8500e+01, -1.8500e+01, -2.1250e+01,\n",
              "         -1.3750e+01,  1.2500e+01, -1.6500e+01, -2.2250e+01,  1.2062e+01,\n",
              "          4.0000e+00, -1.3000e+01,  7.6250e+00, -1.9500e+01,  8.2500e+00,\n",
              "         -5.0000e-01,  4.3000e+01, -2.2500e+00, -1.5625e+01, -3.2250e+01,\n",
              "         -1.5250e+01,  2.8250e+01,  3.0000e+01,  1.4562e+01, -3.8750e+00,\n",
              "          5.0000e+00,  5.6562e+00, -9.2500e+00,  1.2438e+01,  9.0000e+00,\n",
              "         -3.0250e+01,  5.8750e+00, -5.0000e+00,  4.5000e+00, -1.7250e+01,\n",
              "          6.0000e+00,  3.3000e+01,  4.1500e+01,  2.6250e+00,  1.6000e+01,\n",
              "          7.5000e-01,  3.3125e+00, -3.0500e+01, -2.0750e+01,  1.5500e+01,\n",
              "         -8.5000e+00, -3.4375e-01]], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_matmul_result - base_matmul_result).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtnJqavMH28Q",
        "outputId": "86ae977f-783d-480c-cebd-6eaf5bcedfa3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14.3750, dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The errors are huge, and this will lead to gigantic error by our models. That's the reason enterprice libraries like BitsandBytes doesn't use this data type but actually they define a special data type called NF4.\n",
        "\n",
        "NF4 works pretty well with LLM due to their nature"
      ],
      "metadata": {
        "id": "SIZZIPe-Hdhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NF4 - Normal Float 4\n",
        "\n",
        "The weights in large neural networks, including LLMs, tend to follow a zero-centered normal distribution. This means most weights are clustered around zero, with fewer weights at the extremes. NF4 takes advantage of this by creating a quantization scheme where the \"bins\" or discrete values are not equally spaced. Instead, there are more bins around zero to capture the fine-grained details of the majority of the weights, and fewer, wider bins for the less common outlier weights.\n",
        "\n",
        "\n",
        "This non-uniform approach is more information-theoretically optimal for normally distributed data, as it minimizes the quantization error and preserves the crucial information in the weights that are essential for the model's performance."
      ],
      "metadata": {
        "id": "laFXOHX7GPwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NF4_Quantizer_Blockwise():\n",
        "  def __init__(self,block_size=8):\n",
        "    self.nf4_values = torch.tensor([\n",
        "        -1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0000,\n",
        "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7229,  1.0000\n",
        "    ], dtype=torch.float32) # Precomputed\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def quantize(self,input_tensor):\n",
        "    data_flat = input_tensor.view(-1) # Flatten\n",
        "    num_blocks = (data_flat.numel()+ self.block_size -1) // self.block_size\n",
        "    quantized_data = torch.zeros(num_blocks * (self.block_size//2), dtype=torch.uint8) # Every 8 bit we'll pack together 2 tensor of 4 bit\n",
        "    scales = torch.zeros(num_blocks)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      start = i*self.block_size\n",
        "      end = min((i+1)*self.block_size,data_flat.numel())\n",
        "      block = data_flat[start:end]\n",
        "      scale = block.abs().max() # Get the max value of the block for the scale\n",
        "      if scale == 0:\n",
        "        scale = 1.0\n",
        "      scales[i] = scale # Saving the scale factor for the block\n",
        "\n",
        "      scaled_block = block/scale # Scale the tensor\n",
        "      indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.nf4_values),dim=1) # Find the nearest value\n",
        "      # Combine two 4 bit indices in one uint8 value\n",
        "      # This operation refactor the indices organizing it in group of two [[1,2],[3,4]...]\n",
        "      # Then pack the values of the first column with the second column moving this one 4bit to the left (left bit shift operator)\n",
        "      # For example if the index is 5 (0101) shifting it left will result in 0101 0000 (80)\n",
        "      if indices.numel() % 2 != 0:\n",
        "        # Pad with a dummy value to make the number of elements even\n",
        "        indices = torch.cat((indices, torch.tensor([0], dtype=indices.dtype)))\n",
        "\n",
        "      packed_indices = indices.view(-1,2)\n",
        "      packed_values = packed_indices[:, 0] | (packed_indices[:, 1] << 4)\n",
        "      quantized_data[i * (self.block_size // 2) : i * (self.block_size // 2) + packed_values.numel()] = packed_values\n",
        "    return quantized_data, scales\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scales, original_shape):\n",
        "    num_elements = torch.prod(torch.tensor(original_shape))\n",
        "    dequantized_flat = torch.zeros(num_elements, dtype=torch.float32)\n",
        "\n",
        "    num_blocks = scales.numel()\n",
        "    current_index = 0\n",
        "    for i in range(num_blocks):\n",
        "      start = i * self.block_size\n",
        "      end = min((i+1)*self.block_size, num_elements)\n",
        "      current_block_size = end-start\n",
        "      # How many 8-bit values to unpack for the current block\n",
        "      packed_block_size = (current_block_size + 1) // 2\n",
        "      packed_values = quantized_tensor[current_index:current_index+packed_block_size]\n",
        "      # Unpack the values -> I need to do a bitwise operation the most signifanct bit will be the second index\n",
        "      # The least significant bits will be the first index\n",
        "\n",
        "      index_1 = packed_values & 0x0F\n",
        "      index_2 = (packed_values >> 4) & 0x0F\n",
        "      indices_unpacked =torch.stack([index_1,index_2], dim=1).view(-1)\n",
        "      indices_unpacked = indices_unpacked[:current_block_size]\n",
        "      nf4_block_value = self.nf4_values[indices_unpacked.long()]\n",
        "      dequantized_flat[start:end] = nf4_block_value * scales[i]\n",
        "      current_index += packed_block_size\n",
        "    return dequantized_flat.view(original_shape)"
      ],
      "metadata": {
        "id": "H6752CdaEXB7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = NF4_Quantizer_Blockwise(block_size=BLOCK_SIZE)"
      ],
      "metadata": {
        "id": "hSWoZdmOG8y8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matmul_operation = matmul(quantizer=quantizer)"
      ],
      "metadata": {
        "id": "9ZtfpnlWPnJ4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)"
      ],
      "metadata": {
        "id": "1eA9apnIHBMh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_matmul_result = matmul_operation(input_tensor, weights, weights_quantized=False)"
      ],
      "metadata": {
        "id": "7CqKiqNpPDOm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)\n",
        "dequantized_matmul_result = matmul_operation(input_tensor, quantized_weight, weights_quantized=True,scales=scales, shape=weights.shape)"
      ],
      "metadata": {
        "id": "ES53SjYOHIe_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_matmul_result - base_matmul_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ6DBXxPHOW4",
        "outputId": "55738c2a-d362-47a1-b0aa-34828b26f35b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5625, -0.1875,  4.6250,  0.5000,  1.7500, -2.0312,  3.0000,  3.0000,\n",
              "          2.7500, -5.9688,  0.8750,  5.2500, -1.2344, -0.2500,  0.5000,  1.0000,\n",
              "         -2.5625, -3.2500,  2.6250, -0.5000, -4.1250, -0.4375, -0.5000, -0.5000,\n",
              "         -1.2500, -3.2500,  0.3750, -2.3125, -2.0938, -2.8750,  3.2812,  3.8750,\n",
              "         -0.8125, -0.1250,  0.6250,  3.2500, -1.0000, -1.2500, -0.7500, -2.7500,\n",
              "         -0.5000,  7.8750,  2.0000, -1.5000,  3.1250, -4.0000, -3.8750,  1.8750,\n",
              "          1.8750, -2.3594, -4.4375, -1.0000, -1.7500, -1.6250, -0.6250,  0.2500,\n",
              "         -2.8750,  2.0000, -0.5000, -0.9375, -2.8750,  4.7500, -3.8125, -0.1250,\n",
              "          0.9375,  2.0000,  3.5000,  0.1172, -2.4375, -2.7500,  4.1250,  0.3750,\n",
              "         -0.6250,  2.3750, -0.6250,  2.1250,  3.5000, -2.6250, -6.0000, -2.6250,\n",
              "          2.3750, -1.7500,  2.5000, -0.3125,  2.2500, -1.3125,  2.1250,  1.5000,\n",
              "         -1.9375,  2.5000, -2.9219,  5.1250, -4.4062, -5.2500,  1.2500,  2.0000,\n",
              "         -1.3750,  3.0000, -5.5000,  0.5586,  0.2500, -3.5000, -0.7500,  4.0000,\n",
              "          3.3906,  2.7500, -0.7656, -0.2500,  0.0000, -3.3438, -4.2500, -3.1250,\n",
              "          1.0000, -1.5781,  4.5000,  0.1250,  3.9219, -3.1250, -3.9375,  1.3125,\n",
              "         -2.0625, -4.3750,  1.0000,  2.0000, -0.7500,  5.1250, -3.5625, -1.2500,\n",
              "         -0.1875, -4.2500, -0.5000,  2.2500,  2.2500, -0.2500, -0.3125, -3.7500,\n",
              "         -2.0000,  5.5938, -5.1250,  5.8125, -2.0000,  2.0000, -1.2500, -3.2500,\n",
              "         -0.7500, -2.5000, -0.7500, -1.5000, -0.5156,  1.6250,  0.2500, -0.3438,\n",
              "          1.2500,  2.5000, -2.2500, -2.9688,  0.5625, -6.8750,  2.2500,  4.2500,\n",
              "         10.0000, -2.5000,  3.3125, -2.1406, -1.8828, -1.7500, -2.9375,  0.1875,\n",
              "          1.0000, -1.8125,  1.2500,  4.8750,  0.0000, -2.1875, -2.1250, -1.0000,\n",
              "          0.7031,  6.5000,  1.4844,  1.5000, -2.5625,  2.0000,  2.9375,  4.7500,\n",
              "         -0.5000,  2.3750,  0.3750, -2.1250,  1.6250, -4.8750, -1.2500, -2.2500,\n",
              "          3.5000,  6.5000,  1.0000, -3.3125,  3.3125, -2.2500, -1.7500, -2.8750,\n",
              "          1.1875, -0.5000, -1.8750, -2.7500,  4.6250, -3.1250,  2.9375, -5.7500,\n",
              "         -2.0000,  1.0000, -2.4375,  1.0000, -0.6250, -3.5000,  0.7500, -6.5000,\n",
              "          1.7500, -1.6250, -4.0000,  5.0000, -1.0000, -1.7500,  5.2500, -1.5000,\n",
              "         -1.0000, -0.7500, -1.5000,  0.7500, -4.5000, -3.4375, -3.4688, -3.0000,\n",
              "          1.1250, -0.5000, -3.2656,  4.8750,  1.5078,  3.2500, -0.2656,  2.0000,\n",
              "         -2.2500,  4.0625,  2.2500,  0.0000, -2.5625,  0.1719,  2.5000,  7.2500,\n",
              "         -1.2500,  2.0000,  4.2500,  5.7500,  2.3750,  1.1250,  2.5625, -2.2500,\n",
              "         -4.4375, -4.2500, -0.2500, -2.7500,  2.1250, -2.5625, -3.6250, -3.1094,\n",
              "          3.5000,  0.8750,  1.6250,  6.1250, -1.8750, -1.3125,  1.5000,  5.5000,\n",
              "         -1.2500,  2.5625,  0.5625, -4.5312, -1.8125, -2.9219, -2.2500, -4.7500,\n",
              "          2.2500, -3.3750, -2.5625, -0.2500,  0.7500,  2.7500,  1.4688,  1.0000,\n",
              "          0.5000,  5.0000, -0.7500, -2.4375, -0.6250,  0.2500,  0.5000,  2.0000,\n",
              "          1.1250,  0.5000, -5.6250, -5.1250,  0.7500, -2.7500,  0.2500, -1.2500,\n",
              "         -2.5938,  0.7500, -4.6875,  1.7500,  4.0000,  2.3125, -0.5625, -1.9375,\n",
              "         -0.1562,  0.7500, -3.1250, -7.0000,  1.5000,  2.5000,  0.2500,  4.2500,\n",
              "         -2.5000, -2.0000,  0.5000,  2.1875,  3.8750, -2.4375, -0.7500, -0.6250,\n",
              "         -2.2812, -5.2500, -1.5000,  0.2188, -0.7500, -5.0000, -7.7500,  2.5000,\n",
              "          3.0000,  4.4062,  2.2031, -2.5000, -3.0625, -9.2500, -1.7500, -4.5625,\n",
              "          6.0000,  0.0000,  1.0000, -0.5000,  2.0000, -0.7500,  0.1875,  1.6875,\n",
              "          0.5000,  1.0312, -0.6250,  0.8750, -1.3750, -2.3750,  0.5859,  0.2500,\n",
              "         -7.3750,  0.6875,  0.5000,  0.6250, -1.1250, -1.7500,  2.1875, -0.8750,\n",
              "          3.2500, -0.5000, -0.5000, -3.0000,  1.5000, -1.5000,  4.9375,  2.2500,\n",
              "          7.8438,  4.6250, -3.5000, -2.0000, -3.0000,  0.6250,  2.2500, -0.3750,\n",
              "          3.2500,  1.2500, -0.6250,  0.1250,  3.6250,  1.5000,  1.3750,  1.4375,\n",
              "          3.2500,  0.2500,  2.5000,  5.7500,  5.8750,  0.7500, -1.1562, -2.2500,\n",
              "          2.2500, -5.9375, -1.2500, -6.7500,  1.3750,  4.1250,  2.5000,  0.5000,\n",
              "         -4.6250, -2.2500,  1.7500, -1.7500,  3.5000, -1.1250, -1.0000, -1.0000,\n",
              "         -2.9375, -3.3750, -1.6250, -1.3750,  6.2500,  1.7500, -4.0000,  1.8125,\n",
              "          6.3750,  6.6875, -1.0000,  2.5000, -0.5000,  2.5000,  0.1875,  0.3750,\n",
              "          0.5000,  0.2500,  0.7500, -5.5625,  2.5000, -0.7500, -1.6250,  1.2500,\n",
              "         -1.7500, -2.2500,  1.7188,  3.5000, -3.2500,  5.3750, -1.9375, -3.3750,\n",
              "          5.6875,  2.2344, -4.7500, -3.8906,  1.6250, -0.2500, -3.7500, -2.6250,\n",
              "         -0.3750,  2.5625,  2.9688,  0.0000, -0.5000,  8.8750, -0.7500,  1.6250,\n",
              "         -1.6250,  2.8750,  3.3750, -0.7500, -2.0000, -3.1250, -1.2500,  2.5000,\n",
              "         -1.7500,  1.2500,  0.6875, -2.5000, -1.7188, -1.6250,  2.5000,  0.0000,\n",
              "         -4.2500,  2.2500,  5.1250,  2.4062, -0.0312,  0.4375, -3.0312, -0.6250,\n",
              "         -4.5000,  1.3125, -1.0000, -0.9375,  1.7500, -0.3438, -0.4688, -1.1250,\n",
              "         -3.8750,  1.5000, -1.2500,  4.5000, -0.5000, -1.4688,  1.3750, -3.2500,\n",
              "          0.7500,  1.3750, -0.8750,  4.8750,  6.8125, -3.5000, -7.5000, -4.9375]],\n",
              "       dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_matmul_result - base_matmul_result).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UpMtfCSIKMP",
        "outputId": "345fd0c7-28cb-4764-e871-547a637303ad"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3594, dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error now are more manageable.\n",
        "\n",
        "This algorithm is the one used from bitsandbytes when training QLora, or when you quantize the model to manage the memory requirements.\n",
        "\n",
        "Of course the main advantages is inside a custom kernel and CUDA Optimization that performe the dequantize and the matmul operations directly in one step."
      ],
      "metadata": {
        "id": "ihhCDpHMISVo"
      }
    }
  ]
}