{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FP4 Quantization\n",
        "\n",
        "High-Level Overview of FP4 Quantization for LLMs\n",
        "FP4 (4-bit floating-point) quantization is a technique used to significantly reduce the memory footprint and computational cost of Large Language Models (LLMs). The core idea is to represent the high-precision floating-point numbers (like FP16 or FP32) that make up a model's weights and activations with a much smaller 4-bit floating-point format. This allows you to store a massive model in a fraction of the memory and perform computations faster.\n",
        "\n",
        "Here's a high-level breakdown of how it works:\n",
        "\n",
        "- The Challenge: LLMs are huge. A model like LLaMA-7B has 7 billion parameters, each typically stored as a 16-bit floating-point number. This requires roughly 14 GB of VRAM. This is a lot, and it limits who can run these models. The goal of quantization is to reduce this number.\n",
        "\n",
        "- The Idea: Instead of using 16 bits to represent each number, we'll use only 4 bits. A 4-bit floating-point number has a much smaller range of values it can represent. This is a trade-off: we save memory and compute, but we lose some precision. The challenge is to do this in a way that the model's performance doesn't degrade too much.\n",
        "\n",
        "The Core Process: Quantization and De-Quantization:\n",
        "\n",
        "- Quantization: When a model is loaded, its high-precision weights are converted to the 4-bit format. This involves a scaling factor and a data type conversion. The key is to find the right scaling factor that minimizes the loss of information.\n",
        "\n",
        "- De-Quantization (on-the-fly): During a forward pass (inference), the 4-bit weights are loaded from memory. However, to perform the actual matrix multiplication (the core operation in a Transformer's linear layer), the GPU's hardware often requires higher precision (e.g., FP16). So, the 4-bit weights are de-quantized back to a higher precision on the fly. The matrix multiplication is then performed in this higher precision, and the result is stored.\n",
        "\n",
        "- Handling Outliers: A major issue with quantizing LLMs is the presence of \"outliers.\" These are a few values in the weight or activation tensors that are much larger than the rest. A naive quantization scheme would be dominated by these outliers, making the rest of the values lose all their precision. Solutions like bitsandbytes' FP4 and NF4 handle this by using a small, high-precision representation for these outliers while quantizing the majority of the values to 4-bit. This is a \"mixed-precision\" approach within the 4-bit quantization."
      ],
      "metadata": {
        "id": "eJpvWp9f5Lhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key components of a bitsandbytes-like implementation are:\n",
        "\n",
        "## 4-bit Floating-Point Data Type\n",
        "bnb defined the NF4 - NormalFloat4. For our purpose let's use the standard FP4.\n",
        "In the FP4 format the data are represented as:\n",
        "\n",
        "|sign|exp|exp|mantissa| (E2M1)\n",
        "\n",
        "(-1)^s *(1+m/2)^(2-1) * 2^(exp-bias)\n",
        "\n",
        "sign: +1, -1\n",
        "mantissa: 0, 1\n",
        "exponent: 00, 01, 10, 11 (with bias=1)\n",
        "\n",
        "bias is pretty important because allow us to have negative exponents and managing subnormal numbers, that in deep learning are pretty important.\n",
        "\n",
        "So the total representable range is:\n",
        "\n",
        "[-1x(1+0.5)x(2^2)... +1x(1+0.5)x(2^2)]= [-6 ... 6]\n",
        "\n"
      ],
      "metadata": {
        "id": "_emA_FIB7jW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_E2M1:\n",
        "  '''\n",
        "  class that represent the E2M1 format\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.values = []\n",
        "    for sign in [0,1]:\n",
        "      for exp in range(2**2):\n",
        "        for mantissa in range(2):\n",
        "          if exp==0 and mantissa == 0:\n",
        "            value = 0\n",
        "          else:\n",
        "            exp_val = exp-1\n",
        "            mantissa_val = 1+mantissa*0.5\n",
        "            value = (1 if sign==0 else -1) * mantissa_val * (2**(exp_val))\n",
        "\n",
        "          if value not in self.values:\n",
        "            self.values.append(value)\n",
        "    self.values = sorted(self.values)"
      ],
      "metadata": {
        "id": "-dVzeWS-9T-B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case of E2M1\n",
        "fp4_range = FP4_E2M1()\n",
        "fp4_range.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVmibP1rE43c",
        "outputId": "df2c119b-6f11-49b5-bc10-85ea491ef891"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-6.0,\n",
              " -4.0,\n",
              " -3.0,\n",
              " -2.0,\n",
              " -1.5,\n",
              " -1.0,\n",
              " -0.75,\n",
              " 0,\n",
              " 0.75,\n",
              " 1.0,\n",
              " 1.5,\n",
              " 2.0,\n",
              " 3.0,\n",
              " 4.0,\n",
              " 6.0]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "AjB56cCo6Hun"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_range = torch.tensor(FP4_E2M1().values)\n",
        "fp4_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIfrIkUZgYSd",
        "outputId": "b78356da-7a2a-40c4-86aa-31b3005b2849"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.0000, -4.0000, -3.0000, -2.0000, -1.5000, -1.0000, -0.7500,  0.0000,\n",
              "         0.7500,  1.0000,  1.5000,  2.0000,  3.0000,  4.0000,  6.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Educational FP4 Quantization\n",
        "\n",
        "Very first version.\n",
        "This implementation quickly and simply describes the algorithm from an educational point of view.\n",
        "\n",
        "- The input tensor is taken and flattened to one dimension (flatten operation)\n",
        "\n",
        "- The scale value is computed as absmax()\n",
        "\n",
        "- The entire tensor is scaled\n",
        "\n",
        "- For each value in the tensor (using broadcasting), the closest value is calculated within the bucket of the allowed 4-bit value range\n",
        "\n",
        "- Finally, the quantized data is returned"
      ],
      "metadata": {
        "id": "JPGFcf_NKe1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([0.1,0.2,0.3,0.4]).unsqueeze(1) - torch.tensor([0.1,0.2,0.3,0.4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i9dJNjcd2as",
        "outputId": "7e7ad134-bba3-451f-9f16-3fbf40f50bd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000, -0.1000, -0.2000, -0.3000],\n",
              "        [ 0.1000,  0.0000, -0.1000, -0.2000],\n",
              "        [ 0.2000,  0.1000,  0.0000, -0.1000],\n",
              "        [ 0.3000,  0.2000,  0.1000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_Quantizer():\n",
        "  def __init__(self):\n",
        "    self.fp4_values = torch.tensor(FP4_E2M1().values)\n",
        "  def quantize(self, input_tensor):\n",
        "    block = input_tensor.view(-1) # Flatten\n",
        "    scale = block.abs().max() # Get the max value of the block for the scale\n",
        "    if scale == 0:\n",
        "      return torch.zeros_like(block), scale\n",
        "    scaled_block =block/scale # Scale the tensor\n",
        "\n",
        "\n",
        "    indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.fp4_values),dim=1) # Find the nearest value from the range\n",
        "    quantized_data = self.fp4_values[indices]\n",
        "    ## I'm returning the quantized data. It's not the standard way of doing it. We'll see it in the next implementation\n",
        "    return quantized_data, scale\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scale,original_shape):\n",
        "    t = quantized_tensor*scale\n",
        "    t = t.reshape(original_shape)\n",
        "    return t"
      ],
      "metadata": {
        "id": "_2BevkLr7RtZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer()"
      ],
      "metadata": {
        "id": "Dz4expWi7kcQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.randn((1,512))\n",
        "input_tensor.shape, input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5JKTapg9tZY",
        "outputId": "51ed9371-5e6c-4bff-de70-987e5402cda0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 512]),\n",
              " tensor([[ 4.1789e-02, -1.0357e+00,  1.4939e-01,  7.1623e-01, -6.4241e-01,\n",
              "          -3.8041e-01, -1.2908e+00,  1.7141e-01, -5.3247e-01,  1.2557e+00,\n",
              "          -1.9053e+00, -7.6853e-01, -1.0663e+00,  9.3685e-01,  5.0596e-01,\n",
              "           9.1629e-01, -1.1104e+00, -8.7649e-01, -4.0421e-01, -3.8728e-01,\n",
              "          -1.8048e+00,  5.5232e-01,  2.2793e-01,  6.8236e-01, -5.9425e-02,\n",
              "           6.2779e-01, -8.2173e-01, -1.8350e-01, -2.2424e-02, -1.3844e+00,\n",
              "           6.0245e-01, -1.7137e-01,  3.8091e-01,  4.3450e-02,  1.4742e+00,\n",
              "          -5.2175e-01,  2.0057e-01,  7.9602e-01,  2.2168e-01,  4.7247e-02,\n",
              "           6.2529e-01,  2.0800e+00,  6.3924e-01, -2.3966e-01, -4.9708e-01,\n",
              "           7.9678e-01,  5.3253e-01,  1.6228e-02,  9.0824e-01,  1.5564e+00,\n",
              "          -5.8099e-01, -1.2539e+00,  4.9693e-01,  3.0307e-01,  3.2636e-01,\n",
              "           8.2259e-01,  1.4596e+00, -2.0068e+00,  1.1978e+00, -7.4647e-01,\n",
              "           1.0010e+00,  2.8108e-01,  1.1000e+00,  8.4446e-01,  4.7295e-01,\n",
              "          -7.0670e-01, -9.9185e-01, -1.8392e+00,  4.1646e-02,  1.6634e+00,\n",
              "           1.1600e+00,  2.9722e-01, -1.3499e+00, -1.6101e+00, -8.3810e-01,\n",
              "           3.6641e-01,  1.1232e+00, -2.4270e+00, -1.6654e+00,  9.5645e-01,\n",
              "           3.7439e-01,  5.4787e-01, -8.8161e-01,  2.1765e+00, -6.3896e-01,\n",
              "           5.6497e-01,  1.5899e+00, -1.9988e+00, -1.8889e+00,  9.3625e-01,\n",
              "           1.0814e+00,  1.2474e-01, -4.6094e-01,  1.2354e+00,  6.0506e-01,\n",
              "          -2.9689e-01, -1.1903e-01,  1.1782e+00, -6.6280e-01, -1.4014e-01,\n",
              "           3.6878e-02,  2.7500e-01,  5.6838e-01, -6.4773e-01,  1.2904e+00,\n",
              "          -2.1214e-01, -1.4896e+00,  3.9428e-01,  1.3677e+00, -6.6364e-01,\n",
              "           8.1800e-01, -3.3730e-01,  3.8113e-01, -1.4468e+00,  2.8098e-01,\n",
              "          -1.3401e+00,  1.3056e+00, -1.3439e+00, -7.8085e-01,  1.4616e-02,\n",
              "           9.1108e-01,  8.2325e-01,  2.6163e-01, -1.9374e-01,  1.1748e+00,\n",
              "           7.7446e-01,  7.8117e-01, -6.4958e-01,  1.3049e+00, -7.2920e-02,\n",
              "           3.9511e-01, -1.5749e-01,  7.8930e-01, -2.7904e-01,  5.8739e-01,\n",
              "           2.6711e-01,  6.1497e-01, -1.3507e+00, -1.0572e+00,  1.3149e+00,\n",
              "          -1.0032e+00,  3.3413e-01, -3.3580e+00,  1.4724e+00, -7.5514e-01,\n",
              "          -6.2580e-02,  1.0290e+00,  1.5427e-01, -1.2636e+00,  1.6914e+00,\n",
              "           1.0111e+00,  9.6066e-01, -1.5826e+00, -2.1992e+00, -4.0150e-01,\n",
              "           1.3496e+00, -8.5008e-01,  1.4730e+00, -1.0112e-01,  2.0420e-01,\n",
              "           2.4202e-01, -2.5872e-01,  1.4472e+00,  1.9730e-03, -2.6525e+00,\n",
              "          -6.7845e-01, -1.0702e+00, -9.1804e-01,  6.1897e-01, -3.6629e-01,\n",
              "           3.9773e-01, -1.2938e+00,  8.6937e-01,  1.3697e+00,  4.6904e-01,\n",
              "           9.3140e-02, -7.8291e-02,  2.9754e-01,  4.9926e-01, -5.2955e-01,\n",
              "           5.8295e-01, -1.3570e+00, -1.1469e+00, -6.2056e-01, -6.1096e-02,\n",
              "           1.6523e-01, -1.5340e+00, -1.0441e+00, -1.9778e-01,  1.2616e-01,\n",
              "           6.9920e-01,  1.0262e+00, -3.3147e-01,  1.3778e-01,  5.6183e-01,\n",
              "           4.6601e-01,  1.8165e+00,  7.7545e-01, -2.4655e+00,  1.8241e-01,\n",
              "          -2.6183e-01,  7.8084e-01,  9.5816e-01,  2.2585e-01, -1.7175e-01,\n",
              "           5.1777e-01,  6.7895e-01,  3.2962e-01,  5.5330e-01,  9.4087e-01,\n",
              "          -2.3317e-01,  3.1045e+00,  1.2046e-01,  8.7175e-01, -9.4292e-01,\n",
              "           1.7647e+00, -5.5086e-01, -1.5924e+00, -3.2409e-01,  2.6403e-02,\n",
              "           9.1830e-01,  5.8398e-01,  2.6378e+00,  1.2129e+00,  9.2902e-01,\n",
              "           6.4497e-01,  9.7313e-02,  7.0313e-01,  4.8036e-01,  4.5136e-01,\n",
              "           2.0406e-01, -3.3035e-01,  1.6858e+00, -2.2965e+00, -3.7311e-01,\n",
              "          -1.2692e+00,  6.4732e-01, -4.4908e-01, -1.0706e+00, -8.4870e-01,\n",
              "           1.7149e-01, -2.1632e+00, -1.3213e+00,  6.5630e-03,  2.6637e-01,\n",
              "           4.8715e-01,  6.4854e-01,  4.5204e-01,  3.0604e-01, -8.2010e-01,\n",
              "           1.6688e+00,  2.2168e-01,  1.0114e-01,  8.2923e-01,  7.8133e-02,\n",
              "          -6.2741e-01,  4.7223e-01, -7.6701e-01,  7.9773e-02, -7.7258e-01,\n",
              "          -5.0749e-01,  2.7223e-01, -2.4733e-01,  6.6047e-01,  4.4943e-01,\n",
              "          -3.0064e-01, -1.0231e+00, -4.5094e-01,  9.4679e-01,  6.8077e-01,\n",
              "           5.2931e-01, -1.8278e+00, -1.1180e+00,  1.8871e+00,  1.8340e-01,\n",
              "           3.6297e-01, -8.5484e-01, -1.3553e+00,  1.0265e+00,  1.6535e+00,\n",
              "          -8.1833e-01, -9.4595e-01, -6.9419e-01, -1.0285e-01, -9.4637e-01,\n",
              "           1.1652e+00,  1.5590e-01, -5.7351e-01,  9.3193e-01, -5.0546e-01,\n",
              "           1.4292e-01,  1.4237e+00,  1.4350e+00,  2.9017e-01, -5.5764e-02,\n",
              "          -7.2553e-01, -1.4279e-01,  9.1222e-01, -3.1778e-01, -2.7435e-01,\n",
              "           1.3440e+00,  1.3680e+00, -3.2799e-01, -4.1582e-01,  2.3115e+00,\n",
              "           9.9124e-01, -3.2304e-03, -5.3878e-02,  1.4344e+00, -3.5442e-01,\n",
              "           7.0677e-01, -4.0312e-02,  6.7032e-01, -3.2310e-01,  3.0749e-01,\n",
              "           5.1268e-01, -1.8258e-01, -1.7470e+00, -1.2515e-01, -3.4290e-01,\n",
              "           1.2113e+00, -6.7824e-01,  3.3257e-01, -2.0717e+00,  1.8991e-02,\n",
              "           3.7533e-01,  4.4726e-01, -1.5501e+00, -1.4772e+00,  1.0990e+00,\n",
              "           7.1759e-01,  3.1864e-01, -2.1320e-01, -1.1141e+00,  2.3848e+00,\n",
              "          -6.2389e-01,  1.3347e+00,  1.5095e+00, -3.0702e-01,  2.5218e+00,\n",
              "           5.0640e-01, -1.3584e+00,  2.6907e-02, -6.7076e-01, -9.0581e-01,\n",
              "          -2.6769e-01,  6.3225e-01, -3.2900e+00,  9.8298e-01, -1.5615e-01,\n",
              "           1.3058e+00,  1.4107e+00, -2.0282e+00, -1.7885e-01,  1.4915e+00,\n",
              "          -1.9297e+00,  1.1061e-01,  9.2750e-01, -4.6027e-01, -1.7656e+00,\n",
              "          -1.3068e+00,  1.4134e+00, -1.1553e+00, -7.6584e-01,  9.5854e-01,\n",
              "          -1.4218e+00, -1.6174e-01, -3.4793e-01,  1.1069e+00,  7.4043e-01,\n",
              "          -1.0923e+00,  1.2519e+00,  2.2213e-01,  1.2027e+00,  3.1861e-01,\n",
              "          -6.7133e-01, -7.6572e-02,  1.7142e-02, -7.6298e-01,  3.7054e-01,\n",
              "          -1.4010e-01,  1.5576e+00,  4.6496e-01, -1.5529e+00, -4.0360e-01,\n",
              "          -1.0956e-01, -2.7000e-01,  4.8197e-01,  8.8529e-01,  4.8193e-01,\n",
              "           1.4036e-01, -1.8272e-02,  4.2710e-01, -4.4393e-01,  2.6377e-02,\n",
              "          -8.3298e-01, -1.3662e-01,  7.5657e-02, -1.0971e+00, -1.6485e+00,\n",
              "          -7.2594e-01,  1.3651e+00, -7.9229e-01, -1.8159e-01, -2.4317e-01,\n",
              "          -7.1816e-01,  2.2395e-01, -1.7317e+00,  1.0502e+00,  7.6756e-02,\n",
              "           1.7604e+00, -6.4193e-01,  1.9349e-01,  2.7569e+00,  1.6822e+00,\n",
              "          -3.6037e-01, -2.5962e-01, -1.0342e-01, -1.5917e+00, -2.4469e+00,\n",
              "          -1.1419e+00, -3.0580e-01,  3.2212e-01,  9.1185e-01,  7.6468e-01,\n",
              "          -7.0815e-01,  1.0397e+00, -9.7667e-02,  1.6890e+00,  8.0152e-01,\n",
              "           1.0863e+00,  4.4099e-01,  1.1046e+00,  1.3827e+00, -4.5626e-01,\n",
              "           1.2560e+00,  2.5524e+00,  1.4565e+00,  1.3038e+00,  4.9327e-01,\n",
              "           7.8891e-01,  4.3293e-01,  3.6232e-02,  4.8793e-01, -1.6705e+00,\n",
              "           1.2027e+00,  5.0142e-01, -5.3181e-01, -1.2990e+00, -3.4623e-01,\n",
              "          -7.0595e-01,  6.6757e-01, -3.8381e-01,  1.9497e+00, -1.2863e+00,\n",
              "          -1.6184e-01,  2.0156e-01, -5.8377e-01,  4.3919e-01,  5.9882e-01,\n",
              "           4.9884e-01, -1.5314e+00,  1.6486e+00,  7.8229e-01,  3.7409e-02,\n",
              "          -6.6212e-01, -6.5773e-01,  2.9560e-04, -9.4124e-01, -1.6630e+00,\n",
              "          -1.2546e-01, -4.5150e-01,  7.0043e-01,  7.2738e-01, -5.4436e-01,\n",
              "           3.5791e-02, -5.8306e-01, -1.7618e+00,  9.0581e-01,  5.9450e-01,\n",
              "          -2.3509e-01,  7.7533e-02, -9.7641e-01, -7.4056e-01,  1.1840e+00,\n",
              "           7.0049e-01,  1.0186e+00,  1.1554e+00, -9.8665e-01,  7.9400e-01,\n",
              "           5.3156e-01, -1.8698e+00,  3.9043e-01,  8.4373e-01,  1.6205e-01,\n",
              "          -1.2957e+00,  6.5558e-02, -1.4645e-02, -7.9386e-01, -1.5012e-01,\n",
              "           4.6908e-02,  1.9309e+00,  2.6069e-01,  8.6737e-01,  7.6454e-01,\n",
              "          -3.3719e-01, -1.1088e+00, -2.0699e+00, -3.7435e-01, -9.9404e-01,\n",
              "          -9.8602e-01, -9.7785e-01]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_tensor, scale = quantizer.quantize(input_tensor=input_tensor)\n",
        "quantized_tensor, scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gORigRhX7n3p",
        "outputId": "0d21c1ff-2036-4213-f1b9-c3de0f916a8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "         -0.7500, -0.7500,  0.0000,  0.0000,  0.0000, -0.7500, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.7500, -0.7500,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000, -0.7500,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.0000, -0.7500,  0.7500, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500,  0.0000,  0.7500,  0.0000,  0.0000, -1.0000,  0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.7500,  0.0000,  0.0000,\n",
              "         -0.7500, -0.7500,  0.0000,  0.7500,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.7500,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.7500,\n",
              "          0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.7500, -0.7500,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -0.7500, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.7500,  0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.7500,  0.0000,  0.0000,\n",
              "          0.7500,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,\n",
              "          0.7500,  0.7500,  0.0000,  0.7500,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.7500,  0.7500,\n",
              "         -0.7500,  0.0000,  0.7500, -0.7500,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "         -0.7500,  0.7500,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.7500,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000, -0.7500, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.7500,  0.0000,  0.0000,  0.7500,  0.7500,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,\n",
              "         -0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500, -0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.7500,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000, -0.7500,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -0.7500,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
              " tensor(3.3580))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor = quantizer.dequantize(quantized_tensor,scale, input_tensor.shape)"
      ],
      "metadata": {
        "id": "tHqaUL-h75XT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor-input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWxl3zUu_wCh",
        "outputId": "2e4e657b-b647-4ade-9fe1-20979cf6abef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.1789e-02,  1.0357e+00, -1.4939e-01, -7.1623e-01,  6.4241e-01,\n",
              "          3.8041e-01, -1.2277e+00, -1.7141e-01,  5.3247e-01, -1.2557e+00,\n",
              "         -6.1314e-01,  7.6853e-01,  1.0663e+00, -9.3685e-01, -5.0596e-01,\n",
              "         -9.1629e-01,  1.1104e+00,  8.7649e-01,  4.0421e-01,  3.8728e-01,\n",
              "         -7.1369e-01, -5.5232e-01, -2.2793e-01, -6.8236e-01,  5.9425e-02,\n",
              "         -6.2779e-01,  8.2173e-01,  1.8350e-01,  2.2424e-02, -1.1341e+00,\n",
              "         -6.0245e-01,  1.7137e-01, -3.8091e-01, -4.3450e-02,  1.0442e+00,\n",
              "          5.2175e-01, -2.0057e-01, -7.9602e-01, -2.2168e-01, -4.7247e-02,\n",
              "         -6.2529e-01,  4.3851e-01, -6.3924e-01,  2.3966e-01,  4.9708e-01,\n",
              "         -7.9678e-01, -5.3253e-01, -1.6228e-02, -9.0824e-01,  9.6202e-01,\n",
              "          5.8099e-01,  1.2539e+00, -4.9693e-01, -3.0307e-01, -3.2636e-01,\n",
              "         -8.2259e-01,  1.0589e+00, -5.1163e-01, -1.1978e+00,  7.4647e-01,\n",
              "         -1.0010e+00, -2.8108e-01, -1.1000e+00, -8.4446e-01, -4.7295e-01,\n",
              "          7.0670e-01,  9.9185e-01, -6.7924e-01, -4.1646e-02,  8.5502e-01,\n",
              "         -1.1600e+00, -2.9722e-01, -1.1686e+00, -9.0837e-01,  8.3810e-01,\n",
              "         -3.6641e-01, -1.1232e+00, -9.1459e-02, -8.5311e-01, -9.5645e-01,\n",
              "         -3.7439e-01, -5.4787e-01,  8.8161e-01,  3.4194e-01,  6.3896e-01,\n",
              "         -5.6497e-01,  9.2855e-01, -5.1971e-01, -6.2952e-01, -9.3625e-01,\n",
              "         -1.0814e+00, -1.2474e-01,  4.6094e-01, -1.2354e+00, -6.0506e-01,\n",
              "          2.9689e-01,  1.1903e-01, -1.1782e+00,  6.6280e-01,  1.4014e-01,\n",
              "         -3.6878e-02, -2.7500e-01, -5.6838e-01,  6.4773e-01,  1.2281e+00,\n",
              "          2.1214e-01, -1.0289e+00, -3.9428e-01,  1.1507e+00,  6.6364e-01,\n",
              "         -8.1800e-01,  3.3730e-01, -3.8113e-01, -1.0716e+00, -2.8098e-01,\n",
              "         -1.1783e+00,  1.2129e+00, -1.1746e+00,  7.8085e-01, -1.4616e-02,\n",
              "         -9.1108e-01, -8.2325e-01, -2.6163e-01,  1.9374e-01, -1.1748e+00,\n",
              "         -7.7446e-01, -7.8117e-01,  6.4958e-01,  1.2136e+00,  7.2920e-02,\n",
              "         -3.9511e-01,  1.5749e-01, -7.8930e-01,  2.7904e-01, -5.8739e-01,\n",
              "         -2.6711e-01, -6.1497e-01, -1.1677e+00,  1.0572e+00,  1.2036e+00,\n",
              "          1.0032e+00, -3.3413e-01,  0.0000e+00,  1.0461e+00,  7.5514e-01,\n",
              "          6.2580e-02, -1.0290e+00, -1.5427e-01, -1.2549e+00,  8.2706e-01,\n",
              "         -1.0111e+00, -9.6066e-01, -9.3591e-01, -3.1927e-01,  4.0150e-01,\n",
              "          1.1689e+00,  8.5008e-01,  1.0455e+00,  1.0112e-01, -2.0420e-01,\n",
              "         -2.4202e-01,  2.5872e-01,  1.0713e+00, -1.9730e-03,  1.3405e-01,\n",
              "          6.7845e-01,  1.0702e+00,  9.1804e-01, -6.1897e-01,  3.6629e-01,\n",
              "         -3.9773e-01, -1.2247e+00, -8.6937e-01,  1.1488e+00, -4.6904e-01,\n",
              "         -9.3140e-02,  7.8291e-02, -2.9754e-01, -4.9926e-01,  5.2955e-01,\n",
              "         -5.8295e-01, -1.1614e+00,  1.1469e+00,  6.2056e-01,  6.1096e-02,\n",
              "         -1.6523e-01, -9.8445e-01,  1.0441e+00,  1.9778e-01, -1.2616e-01,\n",
              "         -6.9920e-01, -1.0262e+00,  3.3147e-01, -1.3778e-01, -5.6183e-01,\n",
              "         -4.6601e-01,  7.0198e-01, -7.7545e-01, -5.3007e-02, -1.8241e-01,\n",
              "          2.6183e-01, -7.8084e-01, -9.5816e-01, -2.2585e-01,  1.7175e-01,\n",
              "         -5.1777e-01, -6.7895e-01, -3.2962e-01, -5.5330e-01, -9.4087e-01,\n",
              "          2.3317e-01,  2.5345e-01, -1.2046e-01, -8.7175e-01,  9.4292e-01,\n",
              "          7.5372e-01,  5.5086e-01, -9.2607e-01,  3.2409e-01, -2.6403e-02,\n",
              "         -9.1830e-01, -5.8398e-01, -1.1929e-01, -1.2129e+00, -9.2902e-01,\n",
              "         -6.4497e-01, -9.7313e-02, -7.0313e-01, -4.8036e-01, -4.5136e-01,\n",
              "         -2.0406e-01,  3.3035e-01,  8.3268e-01, -2.2197e-01,  3.7311e-01,\n",
              "         -1.2493e+00, -6.4732e-01,  4.4908e-01,  1.0706e+00,  8.4870e-01,\n",
              "         -1.7149e-01, -3.5529e-01, -1.1971e+00, -6.5630e-03, -2.6637e-01,\n",
              "         -4.8715e-01, -6.4854e-01, -4.5204e-01, -3.0604e-01,  8.2010e-01,\n",
              "          8.4963e-01, -2.2168e-01, -1.0114e-01, -8.2923e-01, -7.8133e-02,\n",
              "          6.2741e-01, -4.7223e-01,  7.6701e-01, -7.9773e-02,  7.7258e-01,\n",
              "          5.0749e-01, -2.7223e-01,  2.4733e-01, -6.6047e-01, -4.4943e-01,\n",
              "          3.0064e-01,  1.0231e+00,  4.5094e-01, -9.4679e-01, -6.8077e-01,\n",
              "         -5.2931e-01, -6.9071e-01,  1.1180e+00,  6.3141e-01, -1.8340e-01,\n",
              "         -3.6297e-01,  8.5484e-01, -1.1632e+00, -1.0265e+00,  8.6497e-01,\n",
              "          8.1833e-01,  9.4595e-01,  6.9419e-01,  1.0285e-01,  9.4637e-01,\n",
              "         -1.1652e+00, -1.5590e-01,  5.7351e-01, -9.3193e-01,  5.0546e-01,\n",
              "         -1.4292e-01,  1.0948e+00,  1.0835e+00, -2.9017e-01,  5.5764e-02,\n",
              "          7.2553e-01,  1.4279e-01, -9.1222e-01,  3.1778e-01,  2.7435e-01,\n",
              "          1.1744e+00,  1.1504e+00,  3.2799e-01,  4.1582e-01,  2.0694e-01,\n",
              "         -9.9124e-01,  3.2304e-03,  5.3878e-02,  1.0841e+00,  3.5442e-01,\n",
              "         -7.0677e-01,  4.0312e-02, -6.7032e-01,  3.2310e-01, -3.0749e-01,\n",
              "         -5.1268e-01,  1.8258e-01, -7.7146e-01,  1.2515e-01,  3.4290e-01,\n",
              "         -1.2113e+00,  6.7824e-01, -3.3257e-01, -4.4680e-01, -1.8991e-02,\n",
              "         -3.7533e-01, -4.4726e-01, -9.6832e-01, -1.0412e+00, -1.0990e+00,\n",
              "         -7.1759e-01, -3.1864e-01,  2.1320e-01,  1.1141e+00,  1.3364e-01,\n",
              "          6.2389e-01,  1.1837e+00,  1.0090e+00,  3.0702e-01, -3.3617e-03,\n",
              "         -5.0640e-01, -1.1601e+00, -2.6907e-02,  6.7076e-01,  9.0581e-01,\n",
              "          2.6769e-01, -6.3225e-01, -6.7952e-02, -9.8298e-01,  1.5615e-01,\n",
              "          1.2127e+00,  1.1077e+00, -4.9022e-01,  1.7885e-01,  1.0270e+00,\n",
              "         -5.8881e-01, -1.1061e-01, -9.2750e-01,  4.6027e-01, -7.5284e-01,\n",
              "         -1.2117e+00,  1.1051e+00,  1.1553e+00,  7.6584e-01, -9.5854e-01,\n",
              "         -1.0967e+00,  1.6174e-01,  3.4793e-01, -1.1069e+00, -7.4043e-01,\n",
              "          1.0923e+00, -1.2519e+00, -2.2213e-01, -1.2027e+00, -3.1861e-01,\n",
              "          6.7133e-01,  7.6572e-02, -1.7142e-02,  7.6298e-01, -3.7054e-01,\n",
              "          1.4010e-01,  9.6084e-01, -4.6496e-01, -9.6559e-01,  4.0360e-01,\n",
              "          1.0956e-01,  2.7000e-01, -4.8197e-01, -8.8529e-01, -4.8193e-01,\n",
              "         -1.4036e-01,  1.8272e-02, -4.2710e-01,  4.4393e-01, -2.6377e-02,\n",
              "          8.3298e-01,  1.3662e-01, -7.5657e-02,  1.0971e+00, -8.6997e-01,\n",
              "          7.2594e-01,  1.1534e+00,  7.9229e-01,  1.8159e-01,  2.4317e-01,\n",
              "          7.1816e-01, -2.2395e-01, -7.8672e-01, -1.0502e+00, -7.6756e-02,\n",
              "          7.5803e-01,  6.4193e-01, -1.9349e-01, -2.3844e-01,  8.3630e-01,\n",
              "          3.6037e-01,  2.5962e-01,  1.0342e-01, -9.2676e-01, -7.1537e-02,\n",
              "          1.1419e+00,  3.0580e-01, -3.2212e-01, -9.1185e-01, -7.6468e-01,\n",
              "          7.0815e-01, -1.0397e+00,  9.7667e-02,  8.2943e-01, -8.0152e-01,\n",
              "         -1.0863e+00, -4.4099e-01, -1.1046e+00,  1.1358e+00,  4.5626e-01,\n",
              "         -1.2560e+00, -3.3968e-02,  1.0619e+00,  1.2147e+00, -4.9327e-01,\n",
              "         -7.8891e-01, -4.3293e-01, -3.6232e-02, -4.8793e-01, -8.4795e-01,\n",
              "         -1.2027e+00, -5.0142e-01,  5.3181e-01, -1.2195e+00,  3.4623e-01,\n",
              "          7.0595e-01, -6.6757e-01,  3.8381e-01,  5.6877e-01, -1.2322e+00,\n",
              "          1.6184e-01, -2.0156e-01,  5.8377e-01, -4.3919e-01, -5.9882e-01,\n",
              "         -4.9884e-01, -9.8705e-01,  8.6982e-01, -7.8229e-01, -3.7409e-02,\n",
              "          6.6212e-01,  6.5773e-01, -2.9560e-04,  9.4124e-01, -8.5546e-01,\n",
              "          1.2546e-01,  4.5150e-01, -7.0043e-01, -7.2738e-01,  5.4436e-01,\n",
              "         -3.5791e-02,  5.8306e-01, -7.5670e-01, -9.0581e-01, -5.9450e-01,\n",
              "          2.3509e-01, -7.7533e-02,  9.7641e-01,  7.4056e-01, -1.1840e+00,\n",
              "         -7.0049e-01, -1.0186e+00, -1.1554e+00,  9.8665e-01, -7.9400e-01,\n",
              "         -5.3156e-01, -6.4867e-01, -3.9043e-01, -8.4373e-01, -1.6205e-01,\n",
              "         -1.2228e+00, -6.5558e-02,  1.4645e-02,  7.9386e-01,  1.5012e-01,\n",
              "         -4.6908e-02,  5.8753e-01, -2.6069e-01, -8.6737e-01, -7.6454e-01,\n",
              "          3.3719e-01,  1.1088e+00, -4.4860e-01,  3.7435e-01,  9.9404e-01,\n",
              "          9.8602e-01,  9.7785e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_tensor-input_tensor).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTwe5ZZAolB",
        "outputId": "ec91d8e6-29a6-4849-ccb1-65e3d07a746e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6053)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see we have quantized and dequantize the original tensor, and of course we have a loss on the convertion. The average error is not that big in this example\n",
        "\n",
        "In some cases the differences are pretty huge, and this would be worst in case of bigger outliers"
      ],
      "metadata": {
        "id": "z4b2qagSbGKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NB This is really a basic educational implementation that doesn't really optimize the space for the quantization. It just to show the algorithm"
      ],
      "metadata": {
        "id": "D6WJGsisCObY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Blockwise Quantization\n",
        "\n",
        "This approach is simple and it's great, but real library use a more fine grain approach, calculating multiple scale factor base on blocks.\n",
        "\n",
        "Let's change our code to do that"
      ],
      "metadata": {
        "id": "dZqi5yB6dicT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.randn((1,512))\n",
        "input_tensor.shape, input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM_x8tCSiFVE",
        "outputId": "5f3ce1f4-7d9f-4d70-ee03-9d8e6c838fb6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 512]),\n",
              " tensor([[ 1.9134e+00,  6.2684e-01, -5.3194e-01,  8.6158e-04,  2.4018e-01,\n",
              "           6.5365e-01, -1.2006e+00, -9.7287e-01, -8.6038e-01, -8.2020e-01,\n",
              "          -1.4605e+00, -5.5664e-01, -1.1364e+00, -2.1831e+00,  4.7433e-01,\n",
              "           1.4194e+00, -3.1464e-01, -9.2699e-01, -4.6061e-01,  1.5749e+00,\n",
              "          -1.0689e+00,  7.8841e-02, -4.3668e-01,  1.6188e+00, -1.9432e-01,\n",
              "           9.3042e-02,  3.1148e-01, -8.9370e-01, -1.5305e-01, -1.4858e-01,\n",
              "          -1.0182e+00,  9.1295e-01, -1.0882e-01, -7.5600e-01,  7.1207e-01,\n",
              "          -3.4220e-01,  1.6956e+00,  1.2045e+00,  4.9959e-01, -3.3056e-01,\n",
              "          -1.0106e+00, -5.2655e-03,  1.2117e+00, -6.7760e-01,  1.0650e+00,\n",
              "           1.9966e+00, -3.0152e-01,  6.9609e-01, -5.0688e-01, -1.6772e+00,\n",
              "          -1.8749e-01,  1.2643e+00,  6.3555e-01,  1.2632e+00,  9.2076e-02,\n",
              "           4.2368e-01, -2.0519e+00, -5.6756e-01, -2.1367e-02,  1.2719e-01,\n",
              "          -1.1901e+00,  6.1733e-02, -4.7935e-01,  1.6718e+00, -1.3675e+00,\n",
              "           9.2667e-01, -7.4027e-01,  5.9473e-01, -8.6278e-01, -9.6318e-01,\n",
              "          -1.5653e+00, -9.8244e-01,  2.0945e-01,  1.5167e+00,  4.5441e-01,\n",
              "           2.8299e-01, -4.0445e-01, -4.4034e-01, -5.7069e-01, -4.2886e-01,\n",
              "          -1.1917e+00,  5.0872e-01,  4.9082e-01, -8.1185e-01, -2.6418e-01,\n",
              "           2.2216e-01, -5.1127e-02,  9.4805e-01, -4.2781e-01,  1.1523e+00,\n",
              "           5.2524e-01, -6.3446e-01, -6.3300e-01,  1.3007e+00,  1.4575e-01,\n",
              "           2.3452e-01,  9.2873e-01,  1.1153e+00, -2.3432e+00, -7.0369e-02,\n",
              "          -2.7983e-03,  3.2758e-01,  4.3375e-01,  1.8128e-01,  6.8496e-01,\n",
              "          -9.6895e-01,  1.4687e+00, -3.0396e-01, -1.6839e-01, -1.1001e+00,\n",
              "           7.7679e-01,  7.9831e-01, -3.7229e-01,  3.8805e-01, -5.3911e-01,\n",
              "           4.8575e-01, -1.9824e-01,  1.5178e+00,  1.0754e+00,  2.2060e-01,\n",
              "          -2.6780e-01, -4.0818e-01, -1.3291e+00,  4.7655e-01,  8.4191e-01,\n",
              "          -8.3360e-01,  7.4106e-01, -3.6736e-01,  2.5431e-01, -2.9709e-01,\n",
              "          -4.0786e-01,  1.3792e+00, -3.0957e-02,  1.8173e+00,  7.6355e-01,\n",
              "          -1.5024e+00, -5.6749e-01, -1.1182e+00, -3.6661e-01, -3.5425e-01,\n",
              "          -1.8179e+00, -1.1581e+00,  4.4733e-01,  1.3084e-01,  9.9847e-01,\n",
              "           8.6230e-01,  9.0402e-01, -1.2946e+00,  6.9525e-01, -6.5959e-01,\n",
              "           6.3954e-01, -2.7511e+00,  6.2683e-01, -1.0286e+00,  1.1945e+00,\n",
              "          -2.0473e+00,  3.1746e-01,  1.5791e-01, -6.6832e-01,  1.9299e-01,\n",
              "           1.7168e-01,  7.4656e-01, -8.3607e-02,  6.0209e-01, -1.3806e+00,\n",
              "           1.4699e+00, -5.3363e-01, -5.3406e-02, -2.1200e+00,  9.3615e-01,\n",
              "           2.9231e+00, -4.4924e-01,  1.2578e+00,  2.4495e-01, -8.6774e-01,\n",
              "           1.5659e+00,  2.9105e-01,  9.9843e-02, -9.2392e-01, -7.5625e-01,\n",
              "          -8.4741e-02,  2.6508e-01,  1.4490e+00,  4.3773e-01,  6.6403e-01,\n",
              "           9.4519e-01, -2.0715e-01, -1.7584e+00, -1.4553e+00, -3.3740e-02,\n",
              "          -8.9269e-01, -1.3597e+00, -2.2409e+00,  1.0450e+00, -4.1576e-01,\n",
              "          -1.4300e+00, -2.3526e+00,  1.2775e+00,  1.2344e+00, -3.1297e-01,\n",
              "           2.0277e-01,  1.3690e-01,  6.8967e-01,  3.7417e-01,  1.3265e+00,\n",
              "          -2.1158e+00,  9.9091e-01, -1.5302e+00,  1.0332e+00,  7.8433e-01,\n",
              "           2.1428e-01, -1.5761e+00, -5.9469e-01, -1.3349e+00, -4.9575e-01,\n",
              "           1.3507e+00, -1.2238e+00, -3.1603e-01, -1.0730e-01, -4.0778e-01,\n",
              "           9.6142e-01, -1.2029e+00, -1.1861e+00, -1.5370e+00,  1.8798e+00,\n",
              "          -6.8136e-01, -4.0612e-01, -3.8548e-01,  1.9370e+00,  5.6638e-01,\n",
              "          -3.5570e-01,  1.1871e+00, -3.2623e-01,  8.5479e-02,  2.3275e-01,\n",
              "          -8.6539e-02,  3.0529e-01,  1.1277e-01,  5.1223e-01, -8.9085e-01,\n",
              "           4.3518e-01,  6.0571e-01,  6.9540e-01,  7.3343e-01,  4.6152e-01,\n",
              "          -4.0479e-01, -1.4940e-01,  1.0851e+00, -9.1581e-01, -1.6780e+00,\n",
              "          -4.7051e-02,  2.3852e-01, -1.1510e+00, -7.8726e-01, -3.9714e-01,\n",
              "          -3.2059e-01, -2.1982e+00, -1.5623e-01,  1.3089e+00,  3.5827e-01,\n",
              "          -1.1860e+00, -5.6386e-01,  6.2239e-02,  6.2654e-01, -8.8371e-01,\n",
              "          -1.0710e-01,  6.2511e-01,  6.0944e-01, -1.2150e+00,  1.0394e+00,\n",
              "          -1.2643e+00, -5.0344e-01, -1.6443e+00,  2.4530e+00,  6.4347e-01,\n",
              "           1.2167e-01, -1.0651e+00, -1.2506e+00, -1.6392e+00,  4.6059e-01,\n",
              "           1.2569e-01, -7.7304e-02, -7.5300e-01, -2.1088e+00,  8.3774e-01,\n",
              "          -2.2337e+00, -9.1245e-01,  1.6488e-01, -9.9018e-01,  7.7639e-01,\n",
              "          -2.6922e-01, -4.6639e-01,  1.0989e+00,  1.2981e+00,  1.7678e+00,\n",
              "          -1.5911e+00,  1.4275e+00, -6.0961e-01, -9.2600e-01, -7.4664e-01,\n",
              "          -1.4570e+00, -5.4468e-01, -4.5134e-01, -5.7024e-01,  4.8178e-01,\n",
              "          -1.0417e+00, -1.4252e+00,  2.8843e-01, -7.5894e-01, -4.0686e-01,\n",
              "          -1.6480e+00, -8.0372e-01,  2.7078e-01,  1.1990e+00, -3.9731e-01,\n",
              "          -1.6873e+00, -1.6605e+00,  5.8803e-01, -1.8599e+00, -1.0628e+00,\n",
              "           1.3429e+00, -1.0800e+00, -2.8198e-01, -4.1181e-02,  3.7913e-01,\n",
              "           2.2163e-01,  1.0439e+00,  3.3608e-01, -1.9168e+00,  5.2509e-01,\n",
              "           6.9281e-01,  1.0647e+00,  6.0029e-01, -1.7792e+00, -3.1812e-01,\n",
              "          -1.9417e+00,  1.0736e+00,  2.3812e-01,  3.6990e-01,  1.1516e+00,\n",
              "           5.6075e-01, -1.0247e+00,  5.6990e-01, -1.1336e+00,  2.1893e+00,\n",
              "          -9.9001e-01,  4.8192e-02, -1.0562e+00, -6.9913e-01,  1.1397e+00,\n",
              "           4.0504e-01, -6.1145e-01,  7.8600e-01, -5.5423e-02, -1.5602e+00,\n",
              "           1.8824e-01, -1.1878e+00, -6.6770e-01,  7.1398e-01, -1.8744e-01,\n",
              "          -1.0343e+00,  8.0628e-01,  5.7443e-01, -4.4828e-01, -2.0181e+00,\n",
              "           7.3400e-01,  1.3910e+00, -1.0125e+00, -1.5069e+00,  8.6919e-01,\n",
              "           6.2269e-01,  1.0487e+00,  2.4148e-01, -1.5394e+00, -3.0603e-01,\n",
              "           8.5973e-01, -5.0498e-01,  5.0649e-01,  5.3449e-01,  8.1016e-01,\n",
              "          -1.7022e-01,  2.0397e+00, -9.3367e-01, -1.6651e+00, -1.5364e+00,\n",
              "           7.3500e-02, -1.3323e+00,  1.2334e+00,  5.9686e-01,  3.7929e-01,\n",
              "          -1.2631e+00, -9.9489e-03, -6.2785e-01,  1.1894e+00,  1.0810e-01,\n",
              "          -7.8006e-01,  3.6143e-01,  7.8739e-01,  6.6866e-01, -7.3227e-01,\n",
              "          -5.6491e-01,  1.6780e-01,  1.4312e+00, -6.6539e-01,  8.3855e-02,\n",
              "          -6.3591e-01,  4.1860e-02,  1.3702e+00,  6.7240e-01, -9.7637e-01,\n",
              "          -9.4180e-01,  6.9765e-01, -1.3348e-01,  1.0343e-01, -9.8387e-03,\n",
              "           2.2150e-01,  4.7575e-01, -2.7112e-01, -8.9041e-02, -6.9871e-01,\n",
              "          -1.8177e+00,  1.2817e+00, -6.2565e-01,  7.5651e-01, -1.6652e-01,\n",
              "           6.0598e-01, -1.7547e+00,  1.6023e+00,  8.1420e-01,  1.6885e+00,\n",
              "          -9.2400e-01, -9.1847e-01, -3.3801e-01,  6.1031e-01,  2.1452e+00,\n",
              "          -1.1831e+00,  2.3544e+00, -1.5949e+00, -1.2307e-01, -1.1337e-02,\n",
              "           2.7065e-01, -3.9315e-02, -1.7330e+00,  4.9522e-01,  1.2168e+00,\n",
              "           6.6345e-01, -5.9087e-01,  5.8916e-01, -1.2757e+00, -1.0348e+00,\n",
              "          -4.3533e-01, -8.4558e-01,  5.9757e-01, -5.4031e-01,  4.6416e-01,\n",
              "          -2.0653e-01, -4.0299e-01, -4.8566e-01, -5.5634e-01, -2.8068e-01,\n",
              "           5.4745e-01,  5.4558e-01,  1.6554e+00,  4.6391e-01, -3.7737e-01,\n",
              "          -4.0771e-01, -8.5570e-01, -2.1304e+00, -2.8177e-01, -1.2271e-01,\n",
              "           4.3861e-01, -1.7598e-01, -1.7160e+00,  6.8565e-01,  1.1593e+00,\n",
              "           1.1934e+00,  1.0977e-01,  7.5301e-01,  8.8972e-01, -6.3110e-01,\n",
              "           5.4323e-01, -1.4451e-01,  4.5986e-01, -7.9173e-01, -1.0947e+00,\n",
              "           1.3370e+00, -5.1910e-01, -3.9933e-01, -4.4523e-01,  1.2945e+00,\n",
              "          -2.0508e+00, -1.9031e-01,  1.6374e-01,  1.8378e+00,  4.9596e-01,\n",
              "          -1.2914e+00, -7.1179e-01, -1.4169e+00,  2.0343e+00, -1.9854e+00,\n",
              "           1.7129e+00, -6.9188e-01,  4.0482e-01, -1.5787e+00, -9.3485e-01,\n",
              "           6.9849e-01, -2.8487e-01,  6.2926e-01,  1.0758e+00, -1.2465e+00,\n",
              "          -1.4108e-01,  3.9400e-01]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTUcx4doDDl",
        "outputId": "53fb2807-3ba8-41ca-a2e7-fc8aa001f2d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.0000, -4.0000, -3.0000, -2.0000, -1.5000, -1.0000, -0.7500,  0.0000,\n",
              "         0.7500,  1.0000,  1.5000,  2.0000,  3.0000,  4.0000,  6.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FP4_Quantizer_Blockwise():\n",
        "  def __init__(self,block_size=8):\n",
        "    self.fp4_values = torch.tensor(FP4_E2M1().values)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def quantize(self,input_tensor):\n",
        "    data_flat = input_tensor.view(-1) # Flatten\n",
        "    num_blocks = (data_flat.numel()+ self.block_size -1) // self.block_size\n",
        "    quantized_data = torch.zeros(num_blocks * (self.block_size//2), dtype=torch.uint8) # Every 8 bit we'll pack together 2 tensor of 4 bit\n",
        "    scales = torch.zeros(num_blocks)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      start = i*self.block_size\n",
        "      end = min((i+1)*self.block_size,data_flat.numel())\n",
        "      block = data_flat[start:end]\n",
        "      scale = block.abs().max() # Get the max value of the block for the scale\n",
        "      if scale == 0:\n",
        "        scale = 1.0\n",
        "      scales[i] = scale # Saving the scale factor for the block\n",
        "\n",
        "      scaled_block = block/scale # Scale the tensor\n",
        "      indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.fp4_values),dim=1) # Find the nearest value\n",
        "      # Combine two 4 bit indices in one uint8 value\n",
        "      # This operation refactor the indices organizing it in group of two [[1,2],[3,4]...]\n",
        "      # Then pack the values of the first column with the second column moving this one 4bit to the left (left bit shift operator)\n",
        "      # For example if the index is 5 (0101) shifting it left will result in 0101 0000 (80)\n",
        "      if indices.numel() % 2 != 0:\n",
        "        # Pad with a dummy value to make the number of elements even\n",
        "        indices = torch.cat((indices, torch.tensor([0], dtype=indices.dtype)))\n",
        "\n",
        "      packed_indices = indices.view(-1,2)\n",
        "      packed_values = packed_indices[:, 0] | (packed_indices[:, 1] << 4)\n",
        "      quantized_data[i * (self.block_size // 2) : i * (self.block_size // 2) + packed_values.numel()] = packed_values\n",
        "    return quantized_data, scales\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scales, original_shape):\n",
        "    num_elements = torch.prod(torch.tensor(original_shape))\n",
        "    dequantized_flat = torch.zeros(num_elements, dtype=torch.float32)\n",
        "\n",
        "    num_blocks = scales.numel()\n",
        "    current_index = 0\n",
        "    for i in range(num_blocks):\n",
        "      start = i * self.block_size\n",
        "      end = min((i+1)*self.block_size, num_elements)\n",
        "      current_block_size = end-start\n",
        "      # How many 8-bit values to unpack for the current block\n",
        "      packed_block_size = (current_block_size + 1) // 2\n",
        "      packed_values = quantized_tensor[current_index:current_index+packed_block_size]\n",
        "      # Unpack the values -> I need to do a bitwise operation the most signifanct bit will be the second index\n",
        "      # The least significant bits will be the first index\n",
        "\n",
        "      index_1 = packed_values & 0x0F\n",
        "      index_2 = (packed_values >> 4) & 0x0F\n",
        "      indices_unpacked =torch.stack([index_1,index_2], dim=1).view(-1)\n",
        "      indices_unpacked = indices_unpacked[:current_block_size]\n",
        "      fp4_block_value = self.fp4_values[indices_unpacked.long()]\n",
        "      dequantized_flat[start:end] = fp4_block_value * scales[i]\n",
        "      current_index += packed_block_size\n",
        "    return dequantized_flat.view(original_shape)"
      ],
      "metadata": {
        "id": "sPlRWjcaA61S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer_Blockwise(block_size=64)"
      ],
      "metadata": {
        "id": "VuUlTmxqBpXL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_data, scales = quantizer.quantize(input_tensor)"
      ],
      "metadata": {
        "id": "CrMsujC6Bz1Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_data, len(scales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKmouX5UCILH",
        "outputId": "00db0908-4426-49c4-a9fe-3d198f97fcd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([121, 119, 119, 102, 102, 118,  86, 135, 103, 135, 118, 135, 119, 103,\n",
              "         119, 134, 119, 119, 136, 119, 118, 120, 152, 119, 103, 135, 135, 119,\n",
              "         117, 119, 118, 135, 134, 119, 103, 102, 135, 119, 119, 119, 118, 119,\n",
              "         119, 135, 135, 119, 135, 119, 136, 117, 119, 119, 103, 120, 103, 119,\n",
              "         119, 119, 135, 120, 119, 118, 119, 119, 119, 135, 135, 103, 103, 119,\n",
              "         102, 119, 119, 103, 119,  87, 119, 104, 119, 119, 119, 119, 134, 119,\n",
              "         118, 121, 120, 135, 119, 119, 119, 120, 119, 103, 118, 103, 133, 103,\n",
              "         133, 120, 119, 119,  88, 104, 120, 103, 103, 135, 118, 119, 104, 102,\n",
              "         120, 119, 120, 135, 119, 119, 119, 103, 119, 119, 119, 135, 102, 119,\n",
              "         118, 119, 117, 120, 118, 119, 119, 119, 134, 118, 150, 119, 102, 118,\n",
              "         119, 103,  87, 119, 118, 119, 136, 104, 120, 118, 118, 119, 103, 118,\n",
              "         119, 118, 135, 103, 118, 102, 104, 119, 119, 120, 117, 135, 103,  87,\n",
              "         120, 135, 103, 103, 105, 103, 135, 119, 119, 118, 118, 119, 118, 119,\n",
              "         117, 104, 134, 135, 103, 135, 119, 119, 151, 102, 118, 134, 119, 118,\n",
              "         135, 119, 119, 119, 119, 120, 119, 135, 103, 118, 119, 119, 119, 119,\n",
              "         134, 119, 119, 134, 135, 102, 119, 105, 105, 119, 119, 118, 120, 119,\n",
              "         102, 103, 119, 119, 119, 119, 119, 120, 119,  86, 119, 119, 118, 136,\n",
              "         119, 120, 119, 119, 134, 119, 135, 117, 135, 103, 103,  89, 120, 103,\n",
              "         118, 119, 104, 119], dtype=torch.uint8),\n",
              " 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have N scale factor (with N =Tensor Input dimension / block_size)"
      ],
      "metadata": {
        "id": "SrnIyYAPCquy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor = quantizer.dequantize(quantized_data,scales, input_tensor.shape)"
      ],
      "metadata": {
        "id": "uespBbDNC0Ve"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIyX9SWkiZxf",
        "outputId": "0125d887-c93f-4666-ade7-decb29561622"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.1831,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6373, -1.6373,\n",
              "         -1.6373, -1.6373, -1.6373,  0.0000, -1.6373, -2.1831,  0.0000,  1.6373,\n",
              "          0.0000, -1.6373,  0.0000,  1.6373, -1.6373,  0.0000,  0.0000,  1.6373,\n",
              "          0.0000,  0.0000,  0.0000, -1.6373,  0.0000,  0.0000, -1.6373,  1.6373,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  1.6373,  1.6373,  0.0000,  0.0000,\n",
              "         -1.6373,  0.0000,  1.6373,  0.0000,  1.6373,  2.1831,  0.0000,  0.0000,\n",
              "          0.0000, -1.6373,  0.0000,  1.6373,  0.0000,  1.6373,  0.0000,  0.0000,\n",
              "         -2.1831,  0.0000,  0.0000,  0.0000, -1.6373,  0.0000,  0.0000,  1.6373,\n",
              "         -1.7574,  1.7574,  0.0000,  0.0000,  0.0000, -1.7574, -1.7574, -1.7574,\n",
              "          0.0000,  1.7574,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -1.7574,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.7574,\n",
              "          0.0000,  1.7574,  0.0000,  0.0000,  0.0000,  1.7574,  0.0000,  0.0000,\n",
              "          1.7574,  1.7574, -2.3432,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -1.7574,  1.7574,  0.0000,  0.0000, -1.7574,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.7574,  1.7574,  0.0000,\n",
              "          0.0000,  0.0000, -1.7574,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  2.1924,  0.0000,  2.1924,  0.0000, -2.1924,\n",
              "          0.0000, -2.1924,  0.0000,  0.0000, -2.1924, -2.1924,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -2.1924,  0.0000,  0.0000,  0.0000, -2.9231,\n",
              "          0.0000,  0.0000,  2.1924, -2.1924,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -2.1924,  2.1924,  0.0000,  0.0000,\n",
              "         -2.1924,  0.0000,  2.9231,  0.0000,  2.1924,  0.0000,  0.0000,  2.1924,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1924,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -2.1924, -2.1924,  0.0000,  0.0000, -2.1924,\n",
              "         -2.3526,  1.7645,  0.0000, -1.7645, -2.3526,  1.7645,  1.7645,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  1.7645, -2.3526,  1.7645, -1.7645,\n",
              "          1.7645,  0.0000,  0.0000, -1.7645,  0.0000, -1.7645,  0.0000,  1.7645,\n",
              "         -1.7645,  0.0000,  0.0000,  0.0000,  1.7645, -1.7645, -1.7645, -1.7645,\n",
              "          1.7645,  0.0000,  0.0000,  0.0000,  1.7645,  0.0000,  0.0000,  1.7645,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.7645,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.7645,\n",
              "         -1.7645, -1.7645,  0.0000,  0.0000, -1.7645,  0.0000,  0.0000,  0.0000,\n",
              "         -2.4530,  0.0000,  1.8397,  0.0000, -1.8397,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -1.8397,  1.8397, -1.8397,  0.0000,\n",
              "         -1.8397,  2.4530,  0.0000,  0.0000, -1.8397, -1.8397, -1.8397,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000, -1.8397,  0.0000, -2.4530,  0.0000,  0.0000,\n",
              "         -1.8397,  0.0000,  0.0000,  0.0000,  1.8397,  1.8397,  1.8397, -1.8397,\n",
              "          1.8397,  0.0000, -1.8397,  0.0000, -1.8397,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000, -1.8397, -1.8397,  0.0000,  0.0000,  0.0000, -1.8397,  0.0000,\n",
              "          0.0000,  1.8397,  0.0000, -1.8397, -1.8397,  0.0000, -1.8397, -1.8397,\n",
              "          1.6420, -1.6420,  0.0000,  0.0000,  0.0000,  0.0000,  1.6420,  0.0000,\n",
              "         -2.1893,  0.0000,  0.0000,  1.6420,  0.0000, -1.6420,  0.0000, -2.1893,\n",
              "          1.6420,  0.0000,  0.0000,  1.6420,  0.0000, -1.6420,  0.0000, -1.6420,\n",
              "          2.1893, -1.6420,  0.0000, -1.6420,  0.0000,  1.6420,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -1.6420,  0.0000, -1.6420,  0.0000,  0.0000,  0.0000,\n",
              "         -1.6420,  0.0000,  0.0000,  0.0000, -2.1893,  0.0000,  1.6420, -1.6420,\n",
              "         -1.6420,  1.6420,  0.0000,  1.6420,  0.0000, -1.6420,  0.0000,  1.6420,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1893, -1.6420, -1.6420,\n",
              "         -1.7658,  0.0000, -1.7658,  1.7658,  0.0000,  0.0000, -1.7658,  0.0000,\n",
              "          0.0000,  1.7658,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  1.7658,  0.0000,  0.0000,  0.0000,  0.0000,  1.7658,\n",
              "          0.0000, -1.7658, -1.7658,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -1.7658,  1.7658,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -1.7658,  1.7658,  0.0000,  1.7658, -1.7658, -1.7658,\n",
              "          0.0000,  0.0000,  2.3544, -1.7658,  2.3544, -1.7658,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000, -1.7658,  0.0000,  1.7658,  0.0000,  0.0000,  0.0000,\n",
              "         -1.5978, -1.5978,  0.0000, -1.5978,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.5978,  0.0000,\n",
              "          0.0000,  0.0000, -1.5978, -2.1304,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "         -1.5978,  0.0000,  1.5978,  1.5978,  0.0000,  0.0000,  1.5978,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -1.5978,  1.5978,  0.0000,  0.0000,\n",
              "          0.0000,  1.5978, -2.1304,  0.0000,  0.0000,  1.5978,  0.0000, -1.5978,\n",
              "          0.0000, -1.5978,  2.1304, -2.1304,  1.5978,  0.0000,  0.0000, -1.5978,\n",
              "         -1.5978,  0.0000,  0.0000,  0.0000,  1.5978, -1.5978,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_tensor- input_tensor).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jEoYH4DACq",
        "outputId": "96a91450-47b9-4b60-ae93-9bb7a41c39c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4377)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matmul operations\n",
        "\n",
        "Let's see a problem using the FP4 data type"
      ],
      "metadata": {
        "id": "vUzVNOexDUO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 64"
      ],
      "metadata": {
        "id": "FscvpvuUOqGA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class matmul():\n",
        "  def __init__(self,quantizer):\n",
        "    self.quantizer = quantizer\n",
        "  def __call__(self, input_tensor, weights, scales=None, weights_quantized=False, shape=None):\n",
        "    if weights_quantized:\n",
        "      if shape is None or scales is None:\n",
        "        raise Exception(\"'shape' and 'scales' are required\")\n",
        "      weights = self.quantizer.dequantize(weights, scales, shape)\n",
        "      weights = weights.to(torch.bfloat16)\n",
        "    output = torch.matmul(input_tensor, weights.T)\n",
        "    return output"
      ],
      "metadata": {
        "id": "K90cZWe4OiMh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = FP4_Quantizer_Blockwise(block_size=BLOCK_SIZE)"
      ],
      "metadata": {
        "id": "a3K1p7T5Dv_u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features, out_features = 1024, 512\n",
        "weights = torch.randn(out_features, in_features).to(torch.bfloat16)\n",
        "input_tensor = torch.randn(1, in_features).to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "alca_32YDTkR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matmul_operation = matmul(quantizer = quantizer)"
      ],
      "metadata": {
        "id": "I4u4twe9JwpB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_matmul_result = matmul_operation(input_tensor, weights, weights_quantized=False)"
      ],
      "metadata": {
        "id": "jERTvbLJKBMu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)\n",
        "dequantized_matmul_result = matmul_operation(input_tensor, quantized_weight, weights_quantized=True,scales=scales, shape=weights.shape)"
      ],
      "metadata": {
        "id": "jdBEw6sDDlkO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_matmul_result - base_matmul_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STPY27ZcESNK",
        "outputId": "8d0f7ba4-01b8-4711-b6e5-c19bbf1a5894"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-17.5000, -33.2500,  16.7500,  -6.5312,  16.2500,  20.6250, -25.0000,\n",
              "          -6.5000,  -5.3750, -16.6250,   8.4375,  -2.8750, -18.6250,  -8.5000,\n",
              "         -19.0000,  33.0000,  18.5000,   4.2500,  18.8750,  -7.3750, -13.3750,\n",
              "          -4.2188,  -1.5938,   6.0000,  47.5000,  10.6250,  -7.5000,   0.8750,\n",
              "          -4.1250,  16.3750, -32.2500,  14.0000,   8.8750,  21.2500,  21.7500,\n",
              "          11.0000,  20.7500,   1.0000,   0.5000,  10.2500,  -3.2500, -18.5000,\n",
              "          -6.2500,   7.8125, -31.0000,   2.7500,  15.7500, -13.5625, -31.0000,\n",
              "           0.2500,  27.5000,   8.2500,  20.3750, -24.8750, -19.8750, -13.3750,\n",
              "           0.2500, -30.0000, -40.0000, -11.4375,  22.2500,  -8.3750,  -7.0000,\n",
              "         -13.5000,  -3.6562,  -8.0000,  14.5000, -24.0000, -23.2500,  -8.5625,\n",
              "         -22.5000,  15.7500,  35.0000,   1.8125,  25.5000, -20.6250, -20.5000,\n",
              "           6.2500,  -4.8750,   0.5000, -14.3750, -14.8750,  -3.3125,  -1.5625,\n",
              "         -30.3750,  12.0000,   8.7500, -11.2500, -20.8750,  33.2500,  25.0000,\n",
              "         -14.0000,  45.5000,  -1.8750,  20.5000, -14.7500,   7.5000,   0.1250,\n",
              "          -0.5000,  23.7500,  -1.7500,  10.6250,  12.7500, -11.0000,  14.6250,\n",
              "         -11.7500,  -4.1250, -33.0000, -15.7500,   6.5312, -22.2500,   1.3125,\n",
              "          10.9375, -22.2500,  -4.7500,  15.1250,   3.0625, -24.3750, -12.2500,\n",
              "          20.8750, -46.0000, -19.5000,  -7.6875, -32.5000,  -6.7500,  -8.0000,\n",
              "          -4.6250,  17.5000,  23.8750,  -4.0000, -25.0000,   0.5000,  11.8750,\n",
              "          -8.0625,  30.6250, -24.7500, -14.0000,  25.7500, -28.5000,  -1.3125,\n",
              "         -10.5000,  15.0000, -37.2500,  -0.6250,  -5.0000,  11.8750,  16.3750,\n",
              "           2.0625, -29.6250, -10.2500,   4.2500, -13.2500,  -0.2500,  10.7500,\n",
              "         -23.7500,   2.3125,   4.5000,  -8.3125,   9.6875,   0.2500,  -4.8750,\n",
              "         -16.0000, -16.7500,  14.3750,  -7.0000, -14.2500, -11.5000, -12.0625,\n",
              "         -34.7500,  10.2500,  31.2500,  10.7500,   3.4688,  -3.0000,   8.0000,\n",
              "          -3.8750,  24.5000,   2.0625,  41.5000,  -0.5625, -19.1250,  25.1250,\n",
              "           0.0000,   9.2500,  -8.5000, -18.5000,  -0.7500, -11.0000,  10.2500,\n",
              "          22.8750,   5.0000,  25.0000,  12.2500,  28.5000,   1.1250,   9.0625,\n",
              "           1.7500, -23.5000,  -2.2500,  10.5000,  -6.5000,  -6.2500,   6.7500,\n",
              "         -18.5000,  11.0000,  -9.7500,  21.5000,   9.2500,  23.8750,  45.7500,\n",
              "          11.5000,  -9.5000,  19.7500,  17.2500,  20.6250,  -7.1250,  -3.1875,\n",
              "         -14.1250,  16.7500, -36.0000,  15.1250,  19.3750, -13.8750,  -7.6875,\n",
              "          13.3125,   3.3750,  -9.5000,  -1.5000, -21.7500,  22.5000,   0.5000,\n",
              "           5.1250,  -4.7188,  -6.5000,   7.0000,   2.4688,  25.8750, -17.0000,\n",
              "           2.5000,  -7.5000,  -3.2500, -38.2500,  13.7500, -10.6875,  30.5000,\n",
              "         -18.6250, -29.0000,  -3.0000, -16.5000,   1.1250,  -4.9688,  10.0000,\n",
              "          15.3750,  -4.7500,   2.1719,  -0.5000,  -3.7344,  -2.5000,  20.2500,\n",
              "         -10.5000, -14.3750,  -6.4375, -16.5000, -19.5000, -35.0000,   8.7500,\n",
              "         -37.5000,   4.2500, -27.2500, -44.0000,  16.3750,  26.3750,   3.5000,\n",
              "         -24.8750,   8.7500,  13.3750, -19.0000,  24.8750,  21.5000, -17.5000,\n",
              "          17.8750,  21.1250,  14.7500,  -6.2500,  12.5000, -14.5000,  22.5000,\n",
              "         -19.0000,  11.6875,  -9.3750,  -0.2500,   8.6250, -24.6250,  14.1250,\n",
              "         -16.0000, -10.3750,   4.0000,  18.0000, -17.7500, -18.0000, -21.0000,\n",
              "           6.4062,  -5.0000,   7.0000,   8.3750,  14.5000,  45.0000,  -2.7500,\n",
              "          -9.7500, -47.0000,  -8.2500,  19.3750,  12.8750,  -7.7500,  12.8125,\n",
              "           1.2500,  -6.2500,  -0.5000, -21.3750,  10.7500,  30.5000,   1.7188,\n",
              "          -6.1875, -19.5000,   3.2188,   3.8281,  -5.3750, -21.5000,  15.7500,\n",
              "          -5.8750,  -1.3750,   8.5000, -21.5000, -21.7500,  28.0000,  -6.5000,\n",
              "           9.7500,  15.6250,  -4.0000, -35.2500,  21.7500,  37.0000,  -2.3750,\n",
              "          16.0000,  25.0000, -18.0000,  13.6250, -26.8750,  -2.3750, -12.5000,\n",
              "         -15.1875,   1.6250, -20.5000,   0.5000, -17.5000, -41.0000,  -8.5000,\n",
              "           1.6250,   8.1250,  27.3750,  23.5000,  13.5000,  -5.2500,  -1.7500,\n",
              "          19.7500,  12.0000,  -4.2500,   0.1875,  -2.7500,   1.0000,  -8.8750,\n",
              "         -50.0000, -21.7500, -17.0000, -12.2500,  21.1250,   9.1250,  19.0000,\n",
              "         -11.6250,  -4.0000,   0.2500,   6.7500,  11.2500,  -0.2188,   6.5625,\n",
              "         -20.2500,   6.2812, -15.6250,   7.2500,  20.3750,   8.1875,  -0.6875,\n",
              "         -28.2500, -25.5000,  18.0000,  -4.5000, -16.5000, -10.3750,  24.0000,\n",
              "          -4.7500,  11.5000,   3.6719,  22.0000,  28.7500, -20.0000,  -1.5000,\n",
              "          -5.4375,   9.0000,   7.0000, -15.5000,   5.5625,  -6.5625, -32.5000,\n",
              "          13.2500,  -3.9844, -38.2500,  -2.3750,  14.0000,  21.6250, -11.0000,\n",
              "          14.5000, -31.1250,   0.3750,  12.0625,  -5.2812,  16.7500, -10.5000,\n",
              "          32.0000,  -2.0000,  24.2500, -27.2500,  25.5000, -35.0000,  15.7500,\n",
              "          27.2500, -20.6250, -33.5000,  -5.1875,  -2.0000, -11.1875, -12.3125,\n",
              "          12.8125,  22.8750,  14.3750, -25.2500,  15.3125, -19.5000,  -5.6250,\n",
              "          12.8750,   1.0000,  26.8750,  31.2500,  12.3750,  17.7500,  12.8750,\n",
              "          16.1250, -28.5000,  -4.0000,  -8.7500,  -5.7188,   7.7500,  -1.1875,\n",
              "           2.0000, -11.4375, -26.2500, -16.8750, -36.5000,  12.1250, -22.2500,\n",
              "         -20.5000, -17.2500, -12.5000,  13.5000,  26.8750,  19.0000,  17.8750,\n",
              "          30.2500,  -8.5000,  30.0000,  21.2500,  11.5000, -30.0000, -18.2500,\n",
              "           3.7500,   6.1562,  -1.0000,  -6.8750,   5.2500,  -3.9375, -29.5000,\n",
              "          10.1250, -23.2500,  -3.2500, -13.1250,  -2.1875,  23.2500,  -6.2500,\n",
              "           2.0000, -17.7500,  -0.8125, -25.5000,  21.7500,  -6.2188,  19.1250,\n",
              "          37.0000,  -5.1875,  14.0000, -20.0000,  28.3750,   0.2500, -11.0000,\n",
              "         -12.9375]], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_matmul_result - base_matmul_result).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtnJqavMH28Q",
        "outputId": "327d9816-771f-44ef-8448-fe6ae960400e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14.2500, dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The errors are huge, and this will lead to gigantic error by our models. That's the reason enterprice libraries like BitsandBytes doesn't use this data type but actually they define a special data type called NF4.\n",
        "\n",
        "NF4 works pretty well with LLM due to their nature"
      ],
      "metadata": {
        "id": "SIZZIPe-Hdhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NF4 - Normal Float 4\n",
        "\n",
        "The weights in large neural networks, including LLMs, tend to follow a zero-centered normal distribution. This means most weights are clustered around zero, with fewer weights at the extremes. NF4 takes advantage of this by creating a quantization scheme where the \"bins\" or discrete values are not equally spaced. Instead, there are more bins around zero to capture the fine-grained details of the majority of the weights, and fewer, wider bins for the less common outlier weights.\n",
        "\n",
        "\n",
        "This non-uniform approach is more information-theoretically optimal for normally distributed data, as it minimizes the quantization error and preserves the crucial information in the weights that are essential for the model's performance."
      ],
      "metadata": {
        "id": "laFXOHX7GPwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NF4_Quantizer_Blockwise():\n",
        "  def __init__(self,block_size=8):\n",
        "    self.nf4_values = torch.tensor([\n",
        "        -1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0000,\n",
        "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7229,  1.0000\n",
        "    ], dtype=torch.float32) # Precomputed\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def quantize(self,input_tensor):\n",
        "    data_flat = input_tensor.view(-1) # Flatten\n",
        "    num_blocks = (data_flat.numel()+ self.block_size -1) // self.block_size\n",
        "    quantized_data = torch.zeros(num_blocks * (self.block_size//2), dtype=torch.uint8) # Every 8 bit we'll pack together 2 tensor of 4 bit\n",
        "    scales = torch.zeros(num_blocks)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      start = i*self.block_size\n",
        "      end = min((i+1)*self.block_size,data_flat.numel())\n",
        "      block = data_flat[start:end]\n",
        "      scale = block.abs().max() # Get the max value of the block for the scale\n",
        "      if scale == 0:\n",
        "        scale = 1.0\n",
        "      scales[i] = scale # Saving the scale factor for the block\n",
        "\n",
        "      scaled_block = block/scale # Scale the tensor\n",
        "      indices = torch.argmin(torch.abs(scaled_block.unsqueeze(1)-self.nf4_values),dim=1) # Find the nearest value\n",
        "      # Combine two 4 bit indices in one uint8 value\n",
        "      # This operation refactor the indices organizing it in group of two [[1,2],[3,4]...]\n",
        "      # Then pack the values of the first column with the second column moving this one 4bit to the left (left bit shift operator)\n",
        "      # For example if the index is 5 (0101) shifting it left will result in 0101 0000 (80)\n",
        "      if indices.numel() % 2 != 0:\n",
        "        # Pad with a dummy value to make the number of elements even\n",
        "        indices = torch.cat((indices, torch.tensor([0], dtype=indices.dtype)))\n",
        "\n",
        "      packed_indices = indices.view(-1,2)\n",
        "      packed_values = packed_indices[:, 0] | (packed_indices[:, 1] << 4)\n",
        "      quantized_data[i * (self.block_size // 2) : i * (self.block_size // 2) + packed_values.numel()] = packed_values\n",
        "    return quantized_data, scales\n",
        "\n",
        "  def dequantize(self,quantized_tensor,scales, original_shape):\n",
        "    num_elements = torch.prod(torch.tensor(original_shape))\n",
        "    dequantized_flat = torch.zeros(num_elements, dtype=torch.float32)\n",
        "\n",
        "    num_blocks = scales.numel()\n",
        "    current_index = 0\n",
        "    for i in range(num_blocks):\n",
        "      start = i * self.block_size\n",
        "      end = min((i+1)*self.block_size, num_elements)\n",
        "      current_block_size = end-start\n",
        "      # How many 8-bit values to unpack for the current block\n",
        "      packed_block_size = (current_block_size + 1) // 2\n",
        "      packed_values = quantized_tensor[current_index:current_index+packed_block_size]\n",
        "      # Unpack the values -> I need to do a bitwise operation the most signifanct bit will be the second index\n",
        "      # The least significant bits will be the first index\n",
        "\n",
        "      index_1 = packed_values & 0x0F\n",
        "      index_2 = (packed_values >> 4) & 0x0F\n",
        "      indices_unpacked =torch.stack([index_1,index_2], dim=1).view(-1)\n",
        "      indices_unpacked = indices_unpacked[:current_block_size]\n",
        "      nf4_block_value = self.nf4_values[indices_unpacked.long()]\n",
        "      dequantized_flat[start:end] = nf4_block_value * scales[i]\n",
        "      current_index += packed_block_size\n",
        "    return dequantized_flat.view(original_shape)"
      ],
      "metadata": {
        "id": "H6752CdaEXB7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantizer = NF4_Quantizer_Blockwise(block_size=BLOCK_SIZE)"
      ],
      "metadata": {
        "id": "hSWoZdmOG8y8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matmul_operation = matmul(quantizer=quantizer)"
      ],
      "metadata": {
        "id": "9ZtfpnlWPnJ4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)"
      ],
      "metadata": {
        "id": "1eA9apnIHBMh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_matmul_result = matmul_operation(input_tensor, weights, weights_quantized=False)"
      ],
      "metadata": {
        "id": "7CqKiqNpPDOm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weight, scales = quantizer.quantize(weights)\n",
        "dequantized_matmul_result = matmul_operation(input_tensor, quantized_weight, weights_quantized=True,scales=scales, shape=weights.shape)"
      ],
      "metadata": {
        "id": "ES53SjYOHIe_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_matmul_result - base_matmul_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ6DBXxPHOW4",
        "outputId": "f56cfba6-4581-4122-ba93-8cd307adceb4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7500, -1.5938,  1.7500,  0.5000, -4.0000,  0.2500,  1.2500,  1.7500,\n",
              "         -1.4375, -1.0000, -3.3281, -1.5000,  2.1250,  4.0000,  2.5000,  0.7500,\n",
              "          1.4375, -2.3750,  0.3125, -2.7500, -5.3750, -4.4062,  3.3125, -1.5000,\n",
              "          1.7812, -4.1250, -4.1250,  6.6250,  0.2500,  1.0000,  1.7500, -2.5000,\n",
              "          1.2500, -1.4375, -0.5625, -0.5000,  3.7500, -1.1250,  0.0000,  2.5000,\n",
              "          2.0000,  3.2500,  4.7500, -0.6250,  1.0000, -2.7500,  4.0000,  5.2500,\n",
              "         -1.0625,  0.0000, -0.8750,  0.0000, -1.2500,  0.3750, -2.2500,  1.0000,\n",
              "          2.0000, -0.6250, -0.8750, -4.4688,  2.5000, -0.2500,  3.3750, -0.5000,\n",
              "          1.0938, -5.0000,  1.8750,  0.8750, -5.0000,  3.7500,  4.5000,  4.7500,\n",
              "          2.5000,  3.1875, -5.8750, -5.3750,  0.7500,  0.0000, -2.9375, -3.6250,\n",
              "         -4.3438, -1.2500,  0.6250,  0.8125,  0.5000, -3.0000, -3.0000, -4.2500,\n",
              "          4.4375,  2.6250, -4.0000,  2.5000,  0.2500,  4.2500,  3.5000, -1.2500,\n",
              "         -3.7500, -0.3750, -0.5000,  0.0000, -3.2500, -1.3750, -3.2500,  3.2500,\n",
              "         -1.3750,  5.3750,  2.5000,  3.8750,  5.1875,  1.2500, -0.0625, -6.5000,\n",
              "         -2.0000,  6.0000, -1.0000, -1.2500, -0.3750, -4.2500,  0.7500, -0.6250,\n",
              "         -3.0000,  0.5000,  0.8750,  1.2500,  0.0000,  1.7500, -3.0000, -0.5000,\n",
              "         -1.5625,  0.2500,  4.0000, -3.0000,  1.3750,  3.7812, -2.6875, -1.2500,\n",
              "          3.7500,  2.9062,  4.0000, -7.5938, -7.7500,  0.2500,  4.3750,  2.5000,\n",
              "          0.7500, -1.7500,  0.8125, -4.8750,  0.0000,  0.0000,  2.2500,  6.6562,\n",
              "         -3.0000,  1.5000, -2.7500,  2.2500,  2.7500,  1.3984, -0.6875,  5.7500,\n",
              "          5.7500,  0.1250, -1.6250,  0.4844,  2.5000,  4.1250,  1.2969,  1.5000,\n",
              "         -0.7500,  1.7500,  3.3750, -4.0000, -2.6875, -1.0000, -3.5000, -1.1250,\n",
              "          5.7500,  1.5625,  1.9531,  0.0000,  3.1562, -5.8750, -2.3750, -2.0000,\n",
              "         -2.0000, -2.5000, -4.3125,  4.7500,  1.2500, -4.9375,  1.0000,  0.5938,\n",
              "          0.3750, -0.2500,  2.7500, -4.5938, -0.8750,  4.6250,  1.7500, -1.2500,\n",
              "          0.5000,  4.0000,  1.3438,  3.6875, -4.0625, -1.1875, -2.2500,  1.0000,\n",
              "          5.8750, -2.3438, -3.5000, -1.5000,  2.7500,  0.7500,  2.0000,  3.7500,\n",
              "         -3.2500, -0.2500, -0.6875,  0.0000,  2.7500,  0.9688, -0.2500, -3.5625,\n",
              "          5.0625, -3.5000,  3.5000, -0.5000,  2.2500, -1.5000,  0.5000, -1.0625,\n",
              "         -1.0625,  1.9062,  2.8750, -2.8750,  3.0000, -2.5000, -0.2500,  3.7500,\n",
              "          0.8750,  2.0000,  4.3750,  0.5625,  4.7500, -4.1562,  2.0000,  6.2500,\n",
              "          0.3750, -1.8125,  1.3750, -2.0000, -2.3750, -3.7500, -0.5156,  3.0000,\n",
              "         -4.1875, -0.5000,  5.0000,  1.2500,  0.2500, -4.0625,  1.1875,  4.5000,\n",
              "         -1.8125,  1.2500, -1.7500,  5.2500, -2.7500,  7.3750,  2.2812,  0.6250,\n",
              "          4.2500,  3.0625,  1.2500,  2.1250,  5.0000,  2.1250, -1.5000,  6.2500,\n",
              "         -3.8438, -3.2500, -1.0000, -1.7500,  1.7500,  0.7500,  1.4688,  0.5000,\n",
              "         -1.1250, -0.9375,  3.2500, -0.6250,  3.7500, -1.0000,  1.2500, -3.2500,\n",
              "          0.7500, -3.7500,  7.1250,  2.0312, -1.1250, -2.8750,  0.6250,  2.0000,\n",
              "         -1.2500,  2.5000,  0.5625, -1.7500, -4.5000, -0.2500, -3.7500,  2.1094,\n",
              "         -0.9375, -5.0000,  1.8750,  3.0000,  0.7500,  0.2500, -5.2500,  4.2500,\n",
              "          2.5000, -1.5469,  0.1250,  1.0000, -1.4688,  0.8750, -2.7500, -2.0000,\n",
              "          1.7500,  0.8750, -1.3750, -2.7500, -2.1250,  2.2500, -3.0000, -1.0000,\n",
              "          1.2500,  1.8750, -0.5000, -3.2500,  0.5000, -5.3750, -1.0625, -0.2500,\n",
              "          5.1250,  2.0000, -2.1250,  3.5000, -3.1250,  3.6250,  2.1250, -8.3750,\n",
              "          4.1250, -2.0000, -0.2500,  4.0000, -2.7500,  2.7500,  8.6250,  2.3750,\n",
              "          2.6250,  4.5000,  7.0000,  0.0000,  7.2500,  2.5000, -3.2500, -0.4688,\n",
              "         -4.0000,  4.2500, -2.8750,  2.7500, -4.3125,  1.9375, -5.7500,  7.5000,\n",
              "         -0.9062,  1.9375, -2.0312,  2.0000, -0.8750, -3.6875, -0.5000,  0.9531,\n",
              "          5.7500,  2.0000,  3.9688,  1.9688, -1.7500, -1.7500, -2.4375,  4.7812,\n",
              "         -2.2500, -1.2500, -4.2500, -0.5000,  1.5000,  0.2188, -5.8125, -4.2500,\n",
              "          0.7812, -1.7031,  3.2500,  0.5000,  0.8125, -1.3125,  1.2812, -1.7500,\n",
              "         -5.9375, -3.5000,  1.2656, -1.7500, -3.5000,  3.7500, -0.1328, -2.2500,\n",
              "          2.1562, -7.0625,  3.0000,  4.2500,  2.8750, -0.9062, -0.1250, -7.3125,\n",
              "         -1.0938, -0.7500, -0.3438,  1.6250, -2.0000, -4.5000,  2.0625,  3.6250,\n",
              "          1.0625,  1.5000,  6.3750, -1.0000,  4.5000,  3.7500,  2.2500, -4.5000,\n",
              "         -0.9375,  1.8750, -5.2500,  0.9062,  1.2500, -2.0000,  0.1250,  0.7500,\n",
              "          2.3750,  1.7500, -1.3750, -2.5000, -2.3438, -5.5000,  0.7500,  0.0625,\n",
              "         -9.1875,  3.1250,  3.5938,  3.6562, -1.5000, -2.0625, -2.0000,  0.3750,\n",
              "         -2.5000, -3.7500, -2.2500,  2.5000,  3.2500,  2.0000,  1.1250, -2.2500,\n",
              "         -4.6250,  1.0000,  6.1875, -5.4375,  0.6250, -0.4922, -0.4609, -0.5000,\n",
              "         -0.2500,  3.1250, -0.2500, -0.5000,  4.3125,  2.0000, -3.8750, -1.0000,\n",
              "          1.7656, -1.0000,  3.8750, -1.0000,  7.7500, -5.0000, -0.6250, -4.1250,\n",
              "         -1.0000,  4.5625, -3.2500, -4.0000, -0.7734,  0.1875, -4.1562, -4.0625,\n",
              "         -3.1562, -2.0000,  0.3750, -6.5000, -0.5000, -2.2500,  1.2500, -1.1094]],\n",
              "       dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(dequantized_matmul_result - base_matmul_result).abs().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UpMtfCSIKMP",
        "outputId": "b4e8bad4-5f9a-4e19-a72b-afe3cba6a5dc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.4375, dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error now are more manageable.\n",
        "\n",
        "This algorithm is the one used from bitsandbytes when training QLora, or when you quantize the model to manage the memory requirements.\n",
        "\n",
        "Of course the main advantages is inside a custom kernel and CUDA Optimization that performe the dequantize and the matmul operations directly in one step."
      ],
      "metadata": {
        "id": "ihhCDpHMISVo"
      }
    }
  ]
}